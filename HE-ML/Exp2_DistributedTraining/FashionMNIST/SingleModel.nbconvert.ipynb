{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:39.292810Z",
     "iopub.status.busy": "2023-01-11T15:32:39.292104Z",
     "iopub.status.idle": "2023-01-11T15:32:41.556223Z",
     "shell.execute_reply": "2023-01-11T15:32:41.554385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 15:32:39.688382: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 15:32:39.888258: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-11 15:32:40.649213: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-11 15:32:40.649276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-11 15:32:40.649283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:41.563432Z",
     "iopub.status.busy": "2023-01-11T15:32:41.563033Z",
     "iopub.status.idle": "2023-01-11T15:32:42.115215Z",
     "shell.execute_reply": "2023-01-11T15:32:42.113339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare MNIST dataset\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "x_train, x_val = x_train[:50000], x_train[50000:]\n",
    "y_train, y_val = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# balanced dataset\n",
    "numBatches = 1\n",
    "imagesPerBatch = len(x_train)//numBatches\n",
    "\n",
    "trainBatches = [[0, 0] for i in range(numBatches)]\n",
    "\n",
    "for i in range(numBatches):\n",
    "  idx = imagesPerBatch*i\n",
    "  trainBatches[i][0] = np.subtract(x_train[idx:idx+imagesPerBatch], 128)\n",
    "  trainBatches[i][0].dtype = np.int8\n",
    "  trainBatches[i][1] = np_utils.to_categorical(y_train[idx:idx+imagesPerBatch]).astype(int)*16\n",
    "\n",
    "x_val = np.subtract(x_val, 128)\n",
    "x_val.dtype = np.int8\n",
    "\n",
    "x_test = np.subtract(x_test, 128)\n",
    "x_test.dtype = np.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.122107Z",
     "iopub.status.busy": "2023-01-11T15:32:42.121866Z",
     "iopub.status.idle": "2023-01-11T15:32:42.128486Z",
     "shell.execute_reply": "2023-01-11T15:32:42.126829Z"
    }
   },
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.136725Z",
     "iopub.status.busy": "2023-01-11T15:32:42.136463Z",
     "iopub.status.idle": "2023-01-11T15:32:42.147689Z",
     "shell.execute_reply": "2023-01-11T15:32:42.146127Z"
    }
   },
   "outputs": [],
   "source": [
    "def pocketTanh(matIn, bits, inDims, outDims):\n",
    "    yMax = 128\n",
    "    yMin = -127\n",
    "    joints = [128, 75, 32, -31, -74, -127]\n",
    "    divisor = (1 << bits) * inDims\n",
    "    slopesInv = [yMax, 8, 2, 1, 2, 8, yMax]\n",
    "\n",
    "    matOut = np.full((matIn.shape[0], outDims), yMax)\n",
    "    matActvGradInv = np.full((matIn.shape[0], outDims), slopesInv[0])\n",
    "\n",
    "    for i in range(len(matIn)):\n",
    "      for j in range(len(matIn[i].squeeze())):\n",
    "        x = matIn[i].squeeze()[j] // divisor\n",
    "        if x < joints[0]:\n",
    "          matOut[i][j] = x // 4 + 88\n",
    "          matActvGradInv[i][j] = slopesInv[1]\n",
    "        if x < joints[1]:\n",
    "          matOut[i][j] = x + 32\n",
    "          matActvGradInv[i][j] = slopesInv[2]\n",
    "        if x < joints[2]:\n",
    "          matOut[i][j] = x * 2\n",
    "          matActvGradInv[i][j] = slopesInv[3]\n",
    "        if x < joints[3]:\n",
    "          matOut[i][j] = x - 32\n",
    "          matActvGradInv[i][j] = slopesInv[4]\n",
    "        if x < joints[4]:\n",
    "          matOut[i][j] = x // 4 - 88\n",
    "          matActvGradInv[i][j] = slopesInv[5]\n",
    "        if x < joints[5]:\n",
    "          matOut[i][j] = yMin\n",
    "          matActvGradInv[i][j] = slopesInv[6]\n",
    "    return matOut.astype(int), matActvGradInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.153997Z",
     "iopub.status.busy": "2023-01-11T15:32:42.153752Z",
     "iopub.status.idle": "2023-01-11T15:32:42.164770Z",
     "shell.execute_reply": "2023-01-11T15:32:42.163188Z"
    }
   },
   "outputs": [],
   "source": [
    "def scalarL2Loss(y, yHat):\n",
    "    return (yHat - y) * (yHat - y) // 2\n",
    "\n",
    "def scalarL2LossDelta(y, yHat):\n",
    "    return (yHat - y)\n",
    "\n",
    "def batchL2Loss(yMat, yHatMat):\n",
    "    # IMPORTANT: One sumLoss value per one sample\n",
    "    accumLoss = 0\n",
    "    # Each row corresponds to one input\n",
    "    for i in range(len(yMat)):\n",
    "      columnLoss = 0\n",
    "      for j in range(len(yMat[i])):\n",
    "        columnLoss += scalarL2Loss(yMat[i][j], yHatMat[i][j])\n",
    "      accumLoss += columnLoss\n",
    "    return accumLoss\n",
    "\n",
    "def batchL2LossDelta(yMat, yHatMat):\n",
    "    # Assumption: 1 input -> 1 scalar sumLoss value\n",
    "    # for 1 output of dimention T, lossDeltaMat = (1, T)\n",
    "    lossDeltaMat = np.zeros((yMat.shape[0], yMat.shape[1]))\n",
    "    accumLossDelta = 0;\n",
    "    # Per each input item\n",
    "    for i in range(len(yMat)):\n",
    "      columnLossDelta = 0\n",
    "      for j in range(len(yMat[i])):\n",
    "        scalarLossDelta = scalarL2LossDelta(yMat[i][j], yHatMat[i].squeeze()[j])\n",
    "        lossDeltaMat[i][j] = scalarLossDelta\n",
    "        columnLossDelta += scalarLossDelta\n",
    "      accumLossDelta += columnLossDelta\n",
    "\n",
    "    # return sum! (average is meaningless)\n",
    "    return lossDeltaMat.astype(int), accumLossDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.171243Z",
     "iopub.status.busy": "2023-01-11T15:32:42.170900Z",
     "iopub.status.idle": "2023-01-11T15:32:42.185335Z",
     "shell.execute_reply": "2023-01-11T15:32:42.183902Z"
    }
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size, outLayer = False, debug=False):\n",
    "      self.input_size = input_size\n",
    "      self.output_size = output_size\n",
    "      self.outLayer = outLayer\n",
    "      self.debug = debug\n",
    "      self.weights = np.zeros((input_size, output_size)).astype(int)\n",
    "      self.bias = np.zeros((1, output_size)).astype(int)\n",
    "      self.mDfaWeight = np.zeros((1, 1)).astype(int)\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        dot = self.input @ self.weights\n",
    "        dot += self.bias\n",
    "        output, self.matActvGradInv = pocketTanh(dot, 8, self.input_size, self.output_size)\n",
    "        return output\n",
    "\n",
    "    def backward(self, lastLayerDeltasMat, lrInv):   \n",
    "      mDeltas = self.computeDeltas(lastLayerDeltasMat, lrInv)\n",
    "      batchSize = len(mDeltas) # 1 for one item    \n",
    "      mWeightUpdate = self.input.T @ mDeltas\n",
    "      mWeightUpdate = (mWeightUpdate // lrInv).astype(int)\n",
    "      self.weights -= mWeightUpdate\n",
    "\n",
    "      ones = np.ones((batchSize, 1)).astype(int)\n",
    "      mBiasUpdate = mDeltas.T @ ones\n",
    "      mBiasUpdate = (mBiasUpdate.T // lrInv).astype(int)\n",
    "      self.bias -= mBiasUpdate\n",
    "\n",
    "      return lastLayerDeltasMat\n",
    "\n",
    "    def computeDeltas(self, lastLayerDeltasMat, lrInv):\n",
    "      if self.outLayer:\n",
    "        mDeltas = np.floor_divide(lastLayerDeltasMat, self.matActvGradInv)\n",
    "      else:\n",
    "        dot = lastLayerDeltasMat @ self.mDfaWeight\n",
    "        mDeltas = np.floor_divide(dot, self.matActvGradInv)\n",
    "      return mDeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.192048Z",
     "iopub.status.busy": "2023-01-11T15:32:42.191654Z",
     "iopub.status.idle": "2023-01-11T15:32:42.199900Z",
     "shell.execute_reply": "2023-01-11T15:32:42.198153Z"
    }
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, image):\n",
    "        return image.reshape(image.shape[0], image.shape[1]*image.shape[2])\n",
    "\n",
    "    def backward(self, lastLayerDeltasMat, lrInv):\n",
    "      return lastLayerDeltasMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.207691Z",
     "iopub.status.busy": "2023-01-11T15:32:42.207088Z",
     "iopub.status.idle": "2023-01-11T15:32:42.231614Z",
     "shell.execute_reply": "2023-01-11T15:32:42.229924Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # test\n",
    "    def test(self, x_test, y_test):\n",
    "      # sample dimension first\n",
    "      samples = len(x_test)\n",
    "      corr = 0\n",
    "      for j in range(samples):\n",
    "          # forward propagation\n",
    "          pred = self.predict(x_test[j])\n",
    "\n",
    "          if pred == y_test[j]:\n",
    "            corr += 1\n",
    "      return corr / samples * 100\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, miniBatchSize, lrInv):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        train_accs, val_accs = [], []\n",
    "        maxVal = 0\n",
    "        weights = []\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            sumLoss = 0\n",
    "            sumLossDelta = 0\n",
    "            epochNumCorrect = 0\n",
    "            numIter = int(samples/miniBatchSize)\n",
    "\n",
    "            for j in range(numIter):\n",
    "                batchNumCorrect = 0\n",
    "                idxStart = j * miniBatchSize\n",
    "                idxEnd = idxStart + miniBatchSize\n",
    "\n",
    "                miniBatchImages = x_train[idxStart:idxEnd]\n",
    "                miniBarchTargets = y_train[idxStart:idxEnd]\n",
    "\n",
    "                # forward propagation\n",
    "                output = miniBatchImages\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                  output = layer.forward(output)\n",
    "\n",
    "                sumLoss += batchL2Loss(miniBarchTargets, output)\n",
    "                lossDeltaMat, sumLossDelta = batchL2LossDelta(miniBarchTargets, output)\n",
    "\n",
    "                for r in range(miniBatchSize):\n",
    "                    if miniBarchTargets[r].argmax() == output[r].argmax():\n",
    "                        batchNumCorrect += 1\n",
    "                \n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(lossDeltaMat, lrInv)\n",
    "                \n",
    "                epochNumCorrect += batchNumCorrect;\n",
    "\n",
    "            # Training accuracy\n",
    "            trainAcc = epochNumCorrect/samples * 100\n",
    "            train_accs.append(trainAcc)\n",
    "            # print(\"Epoch: \" + repr(i))\n",
    "            # print(\"Train Accuracy: \" + repr(trainAcc) + \" %\")\n",
    "            \n",
    "            # Validation accuracy\n",
    "            valAcc = self.test(x_val, y_val)\n",
    "            val_accs.append(valAcc)\n",
    "            if len(val_accs) == 1 or val_accs[-1] > maxVal:\n",
    "                weights = [np.copy(self.layers[1].weights), np.copy(self.layers[1].bias), np.copy(self.layers[2].weights), np.copy(self.layers[2].bias)]\n",
    "                # , np.copy(self.layers[3].weights), np.copy(self.layers[3].bias)]\n",
    "                maxVal = valAcc\n",
    "        return train_accs, val_accs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.238836Z",
     "iopub.status.busy": "2023-01-11T15:32:42.238438Z",
     "iopub.status.idle": "2023-01-11T15:32:42.245633Z",
     "shell.execute_reply": "2023-01-11T15:32:42.243859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "samples = 100\n",
    "epochs = 20\n",
    "lrs = [2048, 4096, 8192]\n",
    "bs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.252849Z",
     "iopub.status.busy": "2023-01-11T15:32:42.252263Z",
     "iopub.status.idle": "2023-01-11T15:32:42.261784Z",
     "shell.execute_reply": "2023-01-11T15:32:42.259844Z"
    }
   },
   "outputs": [],
   "source": [
    "DFAWeights1 = np.load(\"DFAWeights1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.269441Z",
     "iopub.status.busy": "2023-01-11T15:32:42.268851Z",
     "iopub.status.idle": "2023-01-11T15:32:42.281635Z",
     "shell.execute_reply": "2023-01-11T15:32:42.279839Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(i, j, lr):\n",
    "  # Net structure\n",
    "  net = Network()\n",
    "  net.add(FlattenLayer())\n",
    "  net.add(FCLayer(28*28, 200))\n",
    "  # net.add(FCLayer(200, 100))\n",
    "  net.add(FCLayer(200, 10, outLayer=True))\n",
    "\n",
    "  net.layers[1].mDfaWeight = DFAWeights1[j]\n",
    "  # net.layers[2].mDfaWeight = DFAWeights2[j]\n",
    "\n",
    "  # Train\n",
    "  _, val_acc, weights = net.fit(trainBatches[i][0], trainBatches[i][1], epochs=epochs, miniBatchSize=bs, lrInv=lr)\n",
    "  \n",
    "  return {str(lr)+'-'+str(j): [val_acc, weights]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.331062Z",
     "iopub.status.busy": "2023-01-11T15:32:42.330474Z",
     "iopub.status.idle": "2023-01-11T17:34:27.065412Z",
     "shell.execute_reply": "2023-01-11T17:34:27.063060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 7304.724001407623 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "res = Parallel(n_jobs=48)(delayed(train)(i, j, lr) for lr in lrs for j in range(samples) for i in range(numBatches))\n",
    "end_time = time.time()\n",
    "print(\"Total time: \" + str(end_time-start_time) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:34:27.073342Z",
     "iopub.status.busy": "2023-01-11T17:34:27.072739Z",
     "iopub.status.idle": "2023-01-11T17:34:27.100041Z",
     "shell.execute_reply": "2023-01-11T17:34:27.098124Z"
    }
   },
   "outputs": [],
   "source": [
    "accsLRSingle = {}\n",
    "for k in range(len(lrs)):\n",
    "    accsMean = []\n",
    "    accsMax = []\n",
    "    W = []\n",
    "    for j in range(samples):\n",
    "        index, dictKey = samples*k+j, str(lrs[k])+'-'+str(j)\n",
    "        max = np.max(res[index][dictKey][0])\n",
    "        mean = np.mean(res[index][dictKey][0])\n",
    "        weights = res[index][dictKey][1]\n",
    "        accsMean.append(mean/100)\n",
    "        accsMax.append(max/100)\n",
    "        W.append(weights)\n",
    "    accsLRSingle[lrs[k]] = [[accsMean, accsMax], W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:34:27.106686Z",
     "iopub.status.busy": "2023-01-11T17:34:27.106347Z",
     "iopub.status.idle": "2023-01-11T17:34:27.621078Z",
     "shell.execute_reply": "2023-01-11T17:34:27.619428Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"accsSingle.pkl\", \"wb\") as f:\n",
    "    pickle.dump(accsLRSingle, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
