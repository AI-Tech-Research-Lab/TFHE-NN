{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import pickle\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:41.563432Z",
     "iopub.status.busy": "2023-01-11T15:32:41.563033Z",
     "iopub.status.idle": "2023-01-11T15:32:42.115215Z",
     "shell.execute_reply": "2023-01-11T15:32:42.113339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare FashionMNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "# Train and Validation sets\n",
    "x_train, x_val = x_train[:50000], x_train[50000:]\n",
    "y_train, y_val = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# Split the train dataset into different batches\n",
    "num_batches = 1\n",
    "images_per_batch = int(len(x_train)/num_batches)\n",
    "\n",
    "train_batches = [[0, 0] for i in range(num_batches)]\n",
    "\n",
    "for i in range(num_batches):\n",
    "    idx = images_per_batch*i\n",
    "    train_batches[i][0] = np.subtract(x_train[idx:idx+images_per_batch], 128)\n",
    "    train_batches[i][0].dtype = np.int8\n",
    "    train_batches[i][1] = np_utils.to_categorical(y_train[idx:idx+images_per_batch]).astype(int)*16\n",
    "\n",
    "# Validation set\n",
    "x_val = np.subtract(x_val, 128)\n",
    "x_val.dtype = np.int8\n",
    "\n",
    "# Test set\n",
    "x_test = np.subtract(x_test, 128)\n",
    "x_test.dtype = np.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.122107Z",
     "iopub.status.busy": "2023-01-11T15:32:42.121866Z",
     "iopub.status.idle": "2023-01-11T15:32:42.128486Z",
     "shell.execute_reply": "2023-01-11T15:32:42.126829Z"
    }
   },
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_weights_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4 + 88\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val + 32\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val - 32\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4 - 88\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Loss Function\n",
    "def L2(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, flatten_in):\n",
    "        return flatten_in.reshape(flatten_in.shape[0], flatten_in.shape[1]*flatten_in.shape[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in):\n",
    "        self.input = fc_in\n",
    "        dot = (self.input @ self.weights) + self.bias       \n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv):   \n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv)\n",
    "        weights_update = self.input.T @ d_DFA      \n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        bias_update = d_DFA.T @ ones      \n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update        \n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.207691Z",
     "iopub.status.busy": "2023-01-11T15:32:42.207088Z",
     "iopub.status.idle": "2023-01-11T15:32:42.231614Z",
     "shell.execute_reply": "2023-01-11T15:32:42.229924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j])\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, mini_batch_size, lr_inv):\n",
    "        train_accs, val_accs, weights = [], [], []\n",
    "        max_val_acc = 0\n",
    "        for i in range(epochs):\n",
    "            epoch_corr = 0\n",
    "            for j in range(int(len(x_train)/mini_batch_size)):\n",
    "                batch_corr = 0\n",
    "                idx_start = j * mini_batch_size\n",
    "                idx_end = idx_start + mini_batch_size\n",
    "\n",
    "                batch_in = x_train[idx_start:idx_end]\n",
    "                batch_target = y_train[idx_start:idx_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                for layer in self.layers:\n",
    "                  batch_in = layer.forward(batch_in)\n",
    "                fwd_out = batch_in               \n",
    "\n",
    "                # Loss\n",
    "                loss = L2(batch_target, fwd_out)\n",
    "\n",
    "                for r in range(mini_batch_size):\n",
    "                    if batch_target[r].argmax() == fwd_out[r].argmax():\n",
    "                        batch_corr += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(loss, lr_inv)\n",
    "                \n",
    "                epoch_corr += batch_corr\n",
    "\n",
    "            acc = epoch_corr/len(x_train) * 100\n",
    "            train_accs.append(acc)\n",
    "\n",
    "            # Validation accuracy\n",
    "            val_acc = self.test(x_val, y_val)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            # Save weights of the best model\n",
    "            if len(val_accs) == 1 or val_accs[-1] > max_val_acc:\n",
    "                weights = [np.copy(self.layers[1].weights), np.copy(self.layers[1].bias), np.copy(self.layers[2].weights), np.copy(self.layers[2].bias)]\n",
    "                max_val_acc = val_acc\n",
    "        return train_accs, val_accs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.238836Z",
     "iopub.status.busy": "2023-01-11T15:32:42.238438Z",
     "iopub.status.idle": "2023-01-11T15:32:42.245633Z",
     "shell.execute_reply": "2023-01-11T15:32:42.243859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters Configuration H\n",
    "bs = 50\n",
    "epochs = 20\n",
    "lrs_inv = [2048, 4096, 8192]\n",
    "\n",
    "# Number of runs\n",
    "runs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.252849Z",
     "iopub.status.busy": "2023-01-11T15:32:42.252263Z",
     "iopub.status.idle": "2023-01-11T15:32:42.261784Z",
     "shell.execute_reply": "2023-01-11T15:32:42.259844Z"
    }
   },
   "outputs": [],
   "source": [
    "## UPLOAD DFA WEIGHTS\n",
    "DFA_weights1 = np.load(\"res/dfa/DFA_weights1.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, run, lr):\n",
    "    # Net structure\n",
    "    net = Network()\n",
    "    net.add(FlattenLayer())\n",
    "    net.add(FCLayer(28*28, 200))\n",
    "    net.add(FCLayer(200, 10, last_layer=True))\n",
    "\n",
    "    net.layers[1].DFA_weights = DFA_weights1[run]\n",
    "\n",
    "    # Train\n",
    "    _, val_acc, weights = net.fit(train_batches[batch][0], train_batches[batch][1], epochs=epochs, mini_batch_size=bs, lr_inv=lr)\n",
    "  \n",
    "    return {str(lr)+'-'+str(run): [val_acc, weights]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T15:32:42.331062Z",
     "iopub.status.busy": "2023-01-11T15:32:42.330474Z",
     "iopub.status.idle": "2023-01-11T17:34:27.065412Z",
     "shell.execute_reply": "2023-01-11T17:34:27.063060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 7304.724001407623 s\n"
     ]
    }
   ],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "start_time = time.time()\n",
    "single_res = Parallel(n_jobs=-1)(delayed(train)(batch, run, lr) for lr in lrs_inv for run in range(runs) for batch in range(num_batches))\n",
    "end_time = time.time()\n",
    "print(\"Total time: \" + str(end_time-start_time) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:34:27.073342Z",
     "iopub.status.busy": "2023-01-11T17:34:27.072739Z",
     "iopub.status.idle": "2023-01-11T17:34:27.100041Z",
     "shell.execute_reply": "2023-01-11T17:34:27.098124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder results of single networks trained on the entire dataset\n",
    "single_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs_mean, accs_max, W  = [], [], []\n",
    "    for runs in range(runs):\n",
    "        index, dict_key = runs*lr_idx+runs, str(lrs_inv[lr_idx])+'-'+str(runs)\n",
    "        accs_mean.append(np.mean(single_res[index][dict_key][0])/100)\n",
    "        accs_max.append(np.max(single_res[index][dict_key][0])/100)\n",
    "        W.append(single_res[index][dict_key][1])\n",
    "    single_nets[lrs_inv[lr_idx]] = [[accs_mean, accs_max], W]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T17:34:27.106686Z",
     "iopub.status.busy": "2023-01-11T17:34:27.106347Z",
     "iopub.status.idle": "2023-01-11T17:34:27.621078Z",
     "shell.execute_reply": "2023-01-11T17:34:27.619428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save single nets\n",
    "with open(\"out/single/accs_single.pkl\", \"wb\") as f:\n",
    "    pickle.dump(single_nets, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
