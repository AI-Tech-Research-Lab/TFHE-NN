{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pycrcnn.he.he import TFHEnuFHE\n",
    "from pycrcnn.he.tfhe_value import TFHEValue\n",
    "from pycrcnn.he.alu import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TernaryMNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Train set\n",
    "x_train = x_train[:, 6:22, 6:22]\n",
    "\n",
    "# Create Ternary classification dataset\n",
    "train_indexes, test_indexes = [], []\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i] == 0 or y_train[i] == 1 or y_train[i] == 2:\n",
    "        train_indexes.append(i)\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] == 0 or y_test[i] == 1 or y_test[i] == 2:\n",
    "        test_indexes.append(i)\n",
    "x_train = np.subtract(x_train[train_indexes], 128)\n",
    "x_train.dtype = np.int8\n",
    "y_train = y_train[train_indexes]\n",
    "\n",
    "val_images = 5000\n",
    "idx_train = len(x_train) - val_images\n",
    "x_train, x_val = x_train[:idx_train], x_train[idx_train:]\n",
    "y_train, y_val = y_train[:idx_train], y_train[idx_train:]\n",
    "y_train = np_utils.to_categorical(y_train).astype(int)*16\n",
    "\n",
    "# Reduced number of images for encrypted training\n",
    "x_train, y_train = x_train[100:150], y_train[100:150]\n",
    "\n",
    "# Test set\n",
    "x_test = x_test[:, 6:22, 6:22]\n",
    "x_test = np.subtract(x_test[test_indexes], 128)\n",
    "x_test.dtype = np.int8\n",
    "y_test = y_test[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_weights_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Loss Function\n",
    "def L2(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual matmul used to check overflow\n",
    "def matmul(m1, m2, check_overflow=False, bits_tfhe = 0, clip_overflow=False, layer=\"\"):\n",
    "    max_value = 2.**(bits_tfhe-1) - 1.\n",
    "    min_value = -2.**(bits_tfhe-1)\n",
    "    res = [[0 for i in range(len(m2[0]))] for j in range(len(m1))]\n",
    "\n",
    "    for i in range(len(m1)):\n",
    "        for j in range(len(m2[0])):\n",
    "            for k in range(len(m2)):\n",
    "                mul = m1[i][k] * m2[k][j]\n",
    "\n",
    "                if clip_overflow:\n",
    "                    mul = max_value if mul>max_value else mul\n",
    "                    mul = min_value if mul<min_value else mul\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if mul>max_value else 0\n",
    "                    overflow += 1 if mul<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"MUL: matmul overflow layer: \" + layer)\n",
    "\n",
    "                res[i][j] += mul\n",
    "\n",
    "                if clip_overflow:\n",
    "                    res[i][j] = max_value if res[i][j]>max_value else res[i][j]\n",
    "                    res[i][j] = min_value if res[i][j]<min_value else res[i][j]\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if res[i][j]>max_value else 0\n",
    "                    overflow += 1 if res[i][j]<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"ADD: matmul overflow layer: \" + layer)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool Layer\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, kernel_size, stride=(1, 1)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, batch, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return np.array([_max(image, self.kernel_size, self.stride) for image in batch])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss\n",
    "\n",
    "def _max(image, kernel_size, stride):\n",
    "    x_s = stride[1]\n",
    "    y_s = stride[0]\n",
    "\n",
    "    x_k = kernel_size[1]\n",
    "    y_k = kernel_size[0]\n",
    "\n",
    "    # print(image)\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "\n",
    "    x_o = ((x_d - x_k) // x_s) + 1\n",
    "    y_o = ((y_d - y_k) // y_s) + 1\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_s\n",
    "        index_column = x * x_s\n",
    "        return matrix[index_row: index_row + y_k, index_column: index_column + x_k]\n",
    "\n",
    "    return [[np.max(get_submatrix(image, x, y).flatten()) for x in range(0, x_o)] for y in range(0, y_o)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, flatten_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return flatten_in.reshape(flatten_in.shape[0], flatten_in.shape[1]*flatten_in.shape[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"fw input=\"  + repr(self.in_dim)\n",
    "        self.input = fc_in\n",
    "\n",
    "        if check_overflow or clip_overflow:\n",
    "            dot = matmul(self.input, self.weights, check_overflow, bits_tfhe, clip_overflow, layer) + self.bias\n",
    "        else:\n",
    "            dot = (self.input @ self.weights) + self.bias\n",
    "\n",
    "        if clip_overflow:\n",
    "            dot = np.clip(dot, min_value, max_value)\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (dot[dot>max_value]).size\n",
    "            overflow += (dot[dot<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: Bias overflow layer: \"  + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):   \n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"bw input=\"  + repr(self.in_dim)\n",
    "\n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (d_DFA[d_DFA>max_value]).size\n",
    "            overflow += (d_DFA[d_DFA<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Deltas overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        if check_overflow or clip_overflow:\n",
    "            weights_update = matmul(self.input.T, d_DFA, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            weights_update = self.input.T @ d_DFA\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (weights_update[weights_update>max_value]).size\n",
    "            overflow += (weights_update[weights_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Weights Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.weights[self.weights>max_value]).size\n",
    "            overflow += (self.weights[self.weights<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: weights overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        if check_overflow or clip_overflow:\n",
    "            bias_update = matmul(d_DFA.T, ones, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            bias_update = d_DFA.T @ ones\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (bias_update[bias_update>max_value]).size\n",
    "            overflow += (bias_update[bias_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Bias Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.bias[self.bias>max_value]).size\n",
    "            overflow += (self.bias[self.bias<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: bias overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False, layer=\"\"):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            if check_overflow or clip_overflow:\n",
    "                dot = matmul(loss, self.DFA_weights, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "            else:\n",
    "                dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j], check_overflow, bits_tfhe, clip_overflow)\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, check_overflow, bits_tfhe, clip_overflow)\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, mini_batch_size, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        train_accs, val_accs = [], []\n",
    "        for i in range(epochs):\n",
    "            epoch_corr = 0\n",
    "            for j in range(int(len(x_train)/mini_batch_size)):\n",
    "                batch_corr = 0\n",
    "                idx_start = j * mini_batch_size\n",
    "                idx_end = idx_start + mini_batch_size\n",
    "\n",
    "                batch_in = x_train[idx_start:idx_end]\n",
    "                batch_target = y_train[idx_start:idx_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                for layer in self.layers:\n",
    "                  batch_in = layer.forward(batch_in, check_overflow, bits_tfhe, clip_overflow)\n",
    "                fwd_out = batch_in               \n",
    "\n",
    "                # Loss\n",
    "                loss = L2(batch_target, fwd_out)\n",
    "\n",
    "                for r in range(mini_batch_size):\n",
    "                    if batch_target[r].argmax() == fwd_out[r].argmax():\n",
    "                        batch_corr += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow)\n",
    "                \n",
    "                epoch_corr += batch_corr\n",
    "\n",
    "            acc = epoch_corr/len(x_train) * 100\n",
    "            train_accs.append(acc)\n",
    "            val_accs.append(self.test(x_val, y_val))\n",
    "            \n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPLOAD DFA WEIGHTS\n",
    "DFA_weights_L1 = np.load(\"res/dfa/DFA_weights_L1.npy\")\n",
    "DFA_weights_L2 = np.load(\"res/dfa/DFA_weights_L2.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypted Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated TFHE-NN\n",
    "aggr_net = Network()\n",
    "aggr_net.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "aggr_net.add(FlattenLayer())\n",
    "aggr_net.add(FCLayer(16, 4))\n",
    "aggr_net.add(FCLayer(4, 2))\n",
    "aggr_net.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "aggr_net.layers[2].DFA_weights = DFA_weights_L1\n",
    "aggr_net.layers[3].DFA_weights = DFA_weights_L2\n",
    "\n",
    "# Load decrypted aggregated weights\n",
    "with open(\"out/aggregation/aggregated_weights.pkl\", \"rb\") as f:\n",
    "    aggr_net.layers[2].weights = pickle.load(f)\n",
    "    aggr_net.layers[2].bias = pickle.load(f)\n",
    "    aggr_net.layers[3].weights = pickle.load(f)\n",
    "    aggr_net.layers[3].bias = pickle.load(f)\n",
    "    aggr_net.layers[4].weights = pickle.load(f)\n",
    "    aggr_net.layers[4].bias = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy aggregated TFHE-NN: 86.48 %\n",
      "Test accuracy aggregated TFHE-NN: 86.68573244359708 %\n"
     ]
    }
   ],
   "source": [
    "# Testing encrypted aggregated network on plain validation data\n",
    "val_acc_enc_aggr = aggr_net.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "# Testing encrypted trained network on plain test data\n",
    "test_acc_enc_aggr = aggr_net.test(x_test, y_test, check_overflow=True, bits_tfhe=22)\n",
    "\n",
    "print(\"Validation accuracy aggregated TFHE-NN: \" + repr(val_acc_enc_aggr) + \" %\")\n",
    "print(\"Test accuracy aggregated TFHE-NN: \" + repr(test_acc_enc_aggr) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2, B2, W3, B3, W4, B4 = [], [], [], [], [], []\n",
    "\n",
    "# Load decrypted trained weights TFHE-NN-1\n",
    "with open(\"out/model1/trained_weights.pkl\", \"rb\") as f:\n",
    "    W2.append(pickle.load(f))\n",
    "    B2.append(pickle.load(f))\n",
    "\n",
    "    W3.append(pickle.load(f))\n",
    "    B3.append(pickle.load(f))\n",
    "\n",
    "    W4.append(pickle.load(f))\n",
    "    B4.append(pickle.load(f))\n",
    "\n",
    "# Load decrypted trained weights TFHE-NN-2\n",
    "with open(\"out/model2/trained_weights.pkl\", \"rb\") as f:\n",
    "    W2.append(pickle.load(f))\n",
    "    B2.append(pickle.load(f))\n",
    "\n",
    "    W3.append(pickle.load(f))\n",
    "    B3.append(pickle.load(f))\n",
    "\n",
    "    W4.append(pickle.load(f))\n",
    "    B4.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated plain NN with plain weights aggregation\n",
    "plain_net = Network()\n",
    "plain_net.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "plain_net.add(FlattenLayer())\n",
    "plain_net.add(FCLayer(16, 4))\n",
    "plain_net.add(FCLayer(4, 2))\n",
    "plain_net.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "# Compute and set the averaged weights\n",
    "plain_net.layers[2].weights = np.mean(W2, axis=0).astype(int)\n",
    "plain_net.layers[2].bias = np.mean(B2, axis=0).astype(int)\n",
    "plain_net.layers[3].weights = np.mean(W3, axis=0).astype(int)\n",
    "plain_net.layers[3].bias = np.mean(B3, axis=0).astype(int)\n",
    "plain_net.layers[4].weights = np.mean(W4, axis=0).astype(int)\n",
    "plain_net.layers[4].bias = np.mean(B4, axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy aggregated plain NN: 86.48 %\n",
      "Test accuracy aggregated plain NN: 86.68573244359708 %\n"
     ]
    }
   ],
   "source": [
    "# Training plain network on plain data\n",
    "val_acc_plain_aggr = plain_net.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "# Testing plain network on plain test data\n",
    "test_acc_plain_aggr = plain_net.test(x_test, y_test, check_overflow=True, bits_tfhe=22)\n",
    "\n",
    "print(\"Validation accuracy aggregated plain NN: \" + repr(val_acc_plain_aggr) + \" %\")\n",
    "print(\"Test accuracy aggregated plain NN: \" + repr(test_acc_plain_aggr) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Encrypted-Plain Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference validation accuracies Encrypted-Plain Aggregated NNs: 0.0%\n",
      "Difference test accuracies Encrypted-Plain Aggregated NNs: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference validation accuracies Encrypted-Plain Aggregated NNs: \" + str(val_acc_enc_aggr-val_acc_plain_aggr) + \"%\")\n",
    "print(\"Difference test accuracies Encrypted-Plain Aggregated NNs: \" + str(test_acc_enc_aggr-test_acc_plain_aggr) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
