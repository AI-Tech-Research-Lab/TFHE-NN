{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pycrcnn.he.HE import TFHEnuFHE\n",
    "from pycrcnn.he.tfhe_value import TFHEValue\n",
    "from pycrcnn.he.alu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScaleZeroPoint(min_val, max_val,num_bits):\n",
    "  # Calc Scale and zero point\n",
    "  qmin = -2.**(num_bits-1)\n",
    "  qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "  scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "  initial_zero_point = qmin - min_val / scale\n",
    "  \n",
    "  zero_point = 0\n",
    "  if initial_zero_point < qmin:\n",
    "      zero_point = qmin\n",
    "  elif initial_zero_point > qmax:\n",
    "      zero_point = qmax\n",
    "  else:\n",
    "      zero_point = initial_zero_point\n",
    "\n",
    "  zero_point = int(zero_point)\n",
    "\n",
    "  return scale, zero_point\n",
    "\n",
    "def quantize_tensor(x, num_bits, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "      min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = -2.**(num_bits-1)\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = q_x.clip(qmin, qmax)\n",
    "    q_x = q_x.astype(float).round().astype(int)\n",
    "    \n",
    "    return q_x\n",
    "  \n",
    "def quantize_tensor2(x, num_bits, min_val=None, max_val=None):\n",
    "    \n",
    "    if not min_val and not max_val: \n",
    "        min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    qmin = -2.**(num_bits-1)\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "\n",
    "    \n",
    "    x = x - min_val          # Allineo tutto l'array in modo che parta da 0\n",
    "    x /= (max_val - min_val) # Lo scalo tra 0 e 1    \n",
    "    x *= (qmax - qmin)       # Lo scalo tra 0 e 16\n",
    "    x -= qmax                # Lo sfaso tra -8 e 7\n",
    "    \n",
    "    q_x = x.astype(float).round().astype(int)\n",
    "    \n",
    "    return q_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Penguins dataset\n",
    "penguins = pd.read_csv('../penguins_size.csv')\n",
    "penguins = penguins.sample(frac=1, random_state=2)\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "x_train, y_train = penguins.loc[:, [\"island\", \"culmen_length_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values, penguins.iloc[:, :1].values\n",
    "for i in range(len(y_train)):\n",
    "  if y_train[i][0] == \"Adelie\":\n",
    "    y_train[i][0] = 0\n",
    "  elif y_train[i][0] == \"Gentoo\":\n",
    "    y_train[i][0] = 1\n",
    "  else:\n",
    "    y_train[i][0] = 2\n",
    "\n",
    "island, sex = {}, {}\n",
    "countI, countS = 0, 0\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "  # Island\n",
    "  if x_train[i][0] in island:\n",
    "    x_train[i][0] = island[x_train[i][0]]\n",
    "  else:\n",
    "    island[x_train[i][0]] = countI\n",
    "    x_train[i][0] = countI\n",
    "    countI += 1\n",
    "\n",
    "x_train[:, 1] = quantize_tensor2(x_train[:, 1], 4)\n",
    "x_train[:, 2] = quantize_tensor2(x_train[:, 2], 4)\n",
    "x_train[:, 3] = quantize_tensor2(x_train[:, 3], 4)\n",
    "\n",
    "train, val = 150, 64\n",
    "x_val, y_val = x_train[train:train+val], y_train[train:train+val]\n",
    "x_test, y_test = x_train[train+val:], y_train[train+val:]\n",
    "y_train = np_utils.to_categorical(y_train).astype(int)*8\n",
    "x_train, y_train = x_train[:train], y_train[:train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3124,
     "status": "ok",
     "timestamp": 1651862255412,
     "user": {
      "displayName": "Luca Colombo",
      "userId": "05787806710317186015"
     },
     "user_tz": -120
    },
    "id": "gJbSks0DFdDs"
   },
   "outputs": [],
   "source": [
    "HE_Client = TFHEnuFHE(16)\n",
    "\n",
    "with open(\"secret_key\", \"rb\") as f:\n",
    "    HE_Client.secret_key = HE_Client.ctx.load_secret_key(f)\n",
    "    \n",
    "with open(\"cloud_key\", \"rb\") as f:\n",
    "    HE_Client.cloud_key = HE_Client.ctx.load_cloud_key(f)\n",
    "\n",
    "cloud_key = HE_Client.cloud_key\n",
    "HE_Client.generate_vm(cloud_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28535,
     "status": "ok",
     "timestamp": 1651862283941,
     "user": {
      "displayName": "Luca Colombo",
      "userId": "05787806710317186015"
     },
     "user_tz": -120
    },
    "id": "tsfVVfwUFeyB"
   },
   "outputs": [],
   "source": [
    "num1 = HE_Client.encrypt(1)\n",
    "num2 = HE_Client.encode(6)\n",
    "sum = num1+num2\n",
    "mul = num1*num2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pocketTanh(matIn, bits, inDims, outDims):\n",
    "    yMax = 128\n",
    "    yMin = -127\n",
    "    joints = [128, 75, 32, -31, -74, -127]\n",
    "    divisor = (1 << bits) * inDims\n",
    "    slopesInv = [yMax, 8, 2, 1, 2, 8, yMax]\n",
    "\n",
    "    matOut = np.full((matIn.shape[0], outDims), yMax)\n",
    "    matActvGradInv = np.full((matIn.shape[0], outDims), slopesInv[0])\n",
    "\n",
    "    for i in range(len(matIn)):\n",
    "      for j in range(len(matIn[i].squeeze())):\n",
    "        x = matIn[i].squeeze()[j] // divisor\n",
    "        if x < joints[0]:\n",
    "          matOut[i][j] = x // 4\n",
    "          matActvGradInv[i][j] = slopesInv[1]\n",
    "        if x < joints[1]:\n",
    "          matOut[i][j] = x\n",
    "          matActvGradInv[i][j] = slopesInv[2]\n",
    "        if x < joints[2]:\n",
    "          matOut[i][j] = x * 2\n",
    "          matActvGradInv[i][j] = slopesInv[3]\n",
    "        if x < joints[3]:\n",
    "          matOut[i][j] = x\n",
    "          matActvGradInv[i][j] = slopesInv[4]\n",
    "        if x < joints[4]:\n",
    "          matOut[i][j] = x // 4\n",
    "          matActvGradInv[i][j] = slopesInv[5]\n",
    "        if x < joints[5]:\n",
    "          matOut[i][j] = yMin\n",
    "          matActvGradInv[i][j] = slopesInv[6]\n",
    "    return matOut.astype(int), matActvGradInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size, outLayer = False, debug=False):\n",
    "      self.input_size = input_size\n",
    "      self.output_size = output_size\n",
    "      self.outLayer = outLayer\n",
    "      self.debug = debug\n",
    "      self.weights = np.zeros((input_size, output_size)).astype(int)\n",
    "      self.bias = np.zeros((1, output_size)).astype(int)\n",
    "      self.mDfaWeight = np.zeros((1, 1))\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, input_data, check_overflow=False, bits_tfhe = 0):\n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        self.input = input_data\n",
    "        dot = np.matmul(self.input, self.weights) + self.bias\n",
    "        # dot = np.clip(dot, SHRT_MIN + 1, SHRT_MAX)\n",
    "        # print(\"DOT:\")\n",
    "        # print(dot)\n",
    "        if check_overflow:\n",
    "          overflow = (dot[dot>max_value]).size\n",
    "          overflow += (dot[dot<min_value]).size\n",
    "          if overflow > 0:\n",
    "            print(\"FC output \"  + repr(self.input_size))\n",
    "            print(overflow)\n",
    "        self.output, self.matActvGradInv = pocketTanh(dot, 8, self.input_size, self.output_size)\n",
    "        # print(\"OUTPUT\")\n",
    "        # print(self.output)\n",
    "        # print(\"ACT GRAD\")\n",
    "        # print(self.matActvGradInv)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, lastLayerDeltasMat, lrInv, check_overflow=False, bits_tfhe=0):      \n",
    "      mDeltas = self.computeDeltas(lastLayerDeltasMat, lrInv)\n",
    "      # print(\"MDELTAS\")\n",
    "      # print(mDeltas)\n",
    "\n",
    "      batchSize = len(mDeltas) # 1 for one item\n",
    "    \n",
    "      mWeightUpdate = np.matmul(self.input.T, mDeltas)\n",
    "\n",
    "      #mWeightUpdate = np.clip(mWeightUpdate, SHRT_MIN + 1, SHRT_MAX)\n",
    "\n",
    "      mWeightUpdate = (mWeightUpdate // lrInv).astype(int)\n",
    "\n",
    "      # print(\"MWEIGHTS UPDATE\")\n",
    "      # print(mWeightUpdate)\n",
    "\n",
    "      self.weights -= mWeightUpdate\n",
    "\n",
    "      max_value = 2.**(bits_tfhe-1) - 1.\n",
    "      min_value = -2.**(bits_tfhe-1)\n",
    "\n",
    "      if check_overflow:\n",
    "          overflow = (self.weights[self.weights>max_value]).size\n",
    "          overflow += (self.weights[self.weights<min_value]).size\n",
    "          if overflow > 0:\n",
    "            print(\"Weights\"  + repr(self.input_size))\n",
    "            print(overflow)\n",
    "\n",
    "      ones = np.ones((batchSize, 1)).astype(int)\n",
    "\n",
    "      mBiasUpdate = np.matmul(mDeltas.T, ones)\n",
    "\n",
    "      mBiasUpdate = np.clip(mBiasUpdate, SHRT_MIN + 1, SHRT_MAX)\n",
    "\n",
    "      mBiasUpdate = (mBiasUpdate.T // lrInv).astype(int)\n",
    "\n",
    "      self.bias -= mBiasUpdate\n",
    "\n",
    "      if check_overflow:\n",
    "          overflow = (self.bias[self.bias>max_value]).size\n",
    "          overflow += (self.bias[self.bias<min_value]).size\n",
    "          if overflow > 0:\n",
    "            print(\"Bias \"  + repr(self.input_size))\n",
    "            print(overflow)\n",
    "\n",
    "      return lastLayerDeltasMat\n",
    "\n",
    "    def setRandomDfaWeight(self, mInDim, mOutDim):\n",
    "      range = isqrt((12 * SHRT_MAX) / (mInDim + mOutDim))\n",
    "      self.mDfaWeight = np.random.randint(-range, range, (mInDim, mOutDim))\n",
    " \n",
    "    def computeDeltas(self, lastLayerDeltasMat, lrInv):\n",
    "      if self.outLayer:\n",
    "        mDeltas = np.floor_divide(lastLayerDeltasMat, self.matActvGradInv)\n",
    "      else:\n",
    "        if self.mDfaWeight.shape[0] != lastLayerDeltasMat.shape[1] and  self.mDfaWeight.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "          print(\"Initialized DFA\")\n",
    "          self.setRandomDfaWeight(lastLayerDeltasMat.shape[1], self.weights.shape[1])\n",
    "        dot = np.matmul(lastLayerDeltasMat, self.mDfaWeight)\n",
    "        mDeltas = np.floor_divide(dot, self.matActvGradInv)\n",
    "      return mDeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # test\n",
    "    def test(self, x_test, y_test, check_overflow=False, bits_tfhe=0):\n",
    "      # sample dimension first\n",
    "      samples = len(x_test)\n",
    "      corr = 0\n",
    "      for j in range(samples):\n",
    "          # forward propagation\n",
    "          pred = self.predict(x_test[j], check_overflow, bits_tfhe)\n",
    "\n",
    "          if pred.argmax() == y_test[j]:\n",
    "            corr += 1\n",
    "      return corr / samples * 100\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data, check_overflow=False, bits_tfhe=0):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, check_overflow, bits_tfhe)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, miniBatchSize, lrInv, check_overflow=False, bits_tfhe=0):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            sumLoss = 0\n",
    "            sumLossDelta = 0\n",
    "            epochNumCorrect = 0\n",
    "            numIter = int(samples/miniBatchSize)\n",
    "\n",
    "            for j in range(numIter):\n",
    "                batchNumCorrect = 0\n",
    "                idxStart = j * miniBatchSize\n",
    "                idxEnd = idxStart + miniBatchSize\n",
    "\n",
    "                miniBatchImages = x_train[idxStart:idxEnd]\n",
    "                miniBarchTargets = y_train[idxStart:idxEnd]\n",
    "\n",
    "                # forward propagation\n",
    "                output = miniBatchImages\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                  output = layer.forward(output, check_overflow=check_overflow, bits_tfhe=bits_tfhe)\n",
    "\n",
    "                sumLoss += batchL2Loss(miniBarchTargets, output)\n",
    "                lossDeltaMat, sumLossDelta = batchL2LossDelta(miniBarchTargets, output)\n",
    "\n",
    "                for r in range(miniBatchSize):\n",
    "                    if miniBarchTargets[r].argmax() == output[r].argmax():\n",
    "                        batchNumCorrect += 1\n",
    "                \n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(lossDeltaMat, lrInv, check_overflow=check_overflow, bits_tfhe=bits_tfhe)\n",
    "                \n",
    "                epochNumCorrect += batchNumCorrect;\n",
    "\n",
    "            # print(\"Epoch: \" + repr(i))\n",
    "            # print(\"SumLoss: \" + repr(sumLoss))\n",
    "            # print(\"EpochNumCorrect: \" + repr(epochNumCorrect))\n",
    "            # print(\"Accuracy: \" + repr(epochNumCorrect/samples * 100) + \" %\")\n",
    "            # print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enc Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1651862283942,
     "user": {
      "displayName": "Luca Colombo",
      "userId": "05787806710317186015"
     },
     "user_tz": -120
    },
    "id": "WqdJOk4QMRVC"
   },
   "outputs": [],
   "source": [
    "def encrypted_pocketTanh(matIn, bits, inDims, outDims):\n",
    "    yMax = HE_Client.encode(128)\n",
    "    yMin = HE_Client.encode(-127)\n",
    "    joints = HE_Client.encode_matrix([128, 75, 32, -31, -74, -127])\n",
    "    divisor = (1 << bits) * inDims\n",
    "    slopesInv = HE_Client.encode_matrix([128, 8, 2, 1, 2, 8, 128])\n",
    "\n",
    "    matOut = np.full((matIn.shape[0], outDims), yMax)\n",
    "    matActvGradInv = np.full((matIn.shape[0], outDims), slopesInv[0])\n",
    "\n",
    "    for i in range(len(matIn)):\n",
    "      for j in range(len(matIn[i].squeeze())):\n",
    "        x = matIn[i].squeeze()[j] / divisor\n",
    "\n",
    "        lt0 = x < joints[0]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt0, (x / 4).value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt0, slopesInv[1].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "        lt1 = x < joints[1]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt1, x.value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt1, slopesInv[2].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "        lt2 = x < joints[2]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt2, (x * 2).value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt2, slopesInv[3].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "        lt3 = x < joints[3]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt3, x.value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt3, slopesInv[4].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "        lt4 = x < joints[4]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt4, (x / 4).value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt4, slopesInv[5].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "        lt5 = x < joints[5]\n",
    "        matOut[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt5, yMin.value, matOut[i][j].value), x.vm, x.n_bits)\n",
    "        matActvGradInv[i][j] = TFHEValue(HE_Client.vm.gate_mux(lt5, slopesInv[6].value, matActvGradInv[i][j].value), x.vm, x.n_bits)\n",
    "\n",
    "    return matOut, matActvGradInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1651862283945,
     "user": {
      "displayName": "Luca Colombo",
      "userId": "05787806710317186015"
     },
     "user_tz": -120
    },
    "id": "M1nEvDcEMRSh"
   },
   "outputs": [],
   "source": [
    "def scalarL2LossDelta(y, yHat):\n",
    "    return (yHat - y)\n",
    "\n",
    "def batchL2LossDelta(yMat, yHatMat):\n",
    "    # Assumption: 1 input -> 1 scalar sumLoss value\n",
    "    # for 1 output of dimention T, lossDeltaMat = (1, T)\n",
    "    lossDeltaMat = np.full((yMat.shape[0], yMat.shape[1]), HE_Client.encode(0))\n",
    "    # Per each input item\n",
    "    for i in range(len(yMat)):\n",
    "      for j in range(len(yMat[i])):\n",
    "        scalarLossDelta = scalarL2LossDelta(yMat[i][j], yHatMat[i].squeeze()[j])\n",
    "        lossDeltaMat[i][j] = scalarLossDelta\n",
    "\n",
    "    # return sum! (average is meaningless)\n",
    "    return lossDeltaMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1651865979417,
     "user": {
      "displayName": "Luca Colombo",
      "userId": "05787806710317186015"
     },
     "user_tz": -120
    },
    "id": "iB6iFZM7MdFV"
   },
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class EncryptedFCLayer:\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size, outLayer = False):\n",
    "      self.input_size = input_size\n",
    "      self.output_size = output_size\n",
    "      self.outLayer = outLayer\n",
    "      self.weights = np.zeros((input_size, output_size)).astype(int)\n",
    "      self.bias = HE_Client.encode_matrix(np.zeros((1, output_size)).astype(int))\n",
    "      self.mDfaWeight = np.zeros((1, 1)).astype(int)\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data        \n",
    "        dot = np.matmul(self.input, self.weights) + self.bias\n",
    "        output, self.matActvGradInv = encrypted_pocketTanh(dot, 8, self.input_size, self.output_size)\n",
    "        return output\n",
    "\n",
    "    def backward(self, lastLayerDeltasMat, lrInv):      \n",
    "      mDeltas = self.computeDeltas(lastLayerDeltasMat, lrInv)\n",
    "      batchSize = len(mDeltas) # 1 for one item\n",
    "\n",
    "      mWeightUpdate = np.matmul(self.input.T, mDeltas)\n",
    "      mWeightUpdate = mWeightUpdate / lrInv\n",
    "      mWeightUpdate = mWeightUpdate.reshape(self.input_size, self.output_size)\n",
    "\n",
    "      print(HE_Client.decrypt_matrix(mWeightUpdate))\n",
    "\n",
    "      if type(self.weights.squeeze()[0][0]) is not TFHEValue:\n",
    "        self.weights = HE_Client.encode_matrix(self.weights)\n",
    "        \n",
    "      self.weights -= mWeightUpdate\n",
    "\n",
    "      ones = np.ones((batchSize, 1)).astype(int)\n",
    "      mBiasUpdate = np.matmul(mDeltas.T, ones)\n",
    "      mBiasUpdate = mBiasUpdate.T / lrInv\n",
    "      self.bias -= mBiasUpdate\n",
    "\n",
    "    def setRandomDfaWeight(self, mInDim, mOutDim):\n",
    "      range = isqrt((12 * SHRT_MAX) / (mInDim + mOutDim))\n",
    "      self.mDfaWeight = np.random.randint(-range, range, (mInDim, mOutDim))\n",
    " \n",
    "    def computeDeltas(self, lastLayerDeltasMat, lrInv):\n",
    "      if self.outLayer:\n",
    "        mDeltas = np.divide(lastLayerDeltasMat, self.matActvGradInv)\n",
    "      else:\n",
    "        if self.mDfaWeight.shape[0] != lastLayerDeltasMat.shape[1] and  self.mDfaWeight.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "          print(\"Initialized DFA\")\n",
    "          self.setRandomDfaWeight(lastLayerDeltasMat.shape[1], self.weights.shape[1])\n",
    "        dot = np.matmul(lastLayerDeltasMat, self.mDfaWeight)   \n",
    "        mDeltas = np.divide(dot, self.matActvGradInv)\n",
    "      return mDeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # serialize the network\n",
    "    def serialize(self):\n",
    "        for l in self.layers:\n",
    "          if hasattr(l, \"weights\"):\n",
    "            l.weights = HE_Client.serialize_matrix(l.weights)\n",
    "            l.bias = HE_Client.serialize_matrix(l.bias)\n",
    "            l.matActvGradInv = None\n",
    "            l.input = None\n",
    "            if not l.outLayer:\n",
    "              l.mDfaWeight = HE_Client.serialize_matrix(l.mDfaWeight)\n",
    "    \n",
    "    # deserialize the network\n",
    "    def deserialize(self):\n",
    "        for l in self.layers:\n",
    "          if hasattr(l, \"weights\"):\n",
    "            l.weights = HE_Client.deserialize_matrix(l.weights)\n",
    "            l.bias = HE_Client.deserialize_matrix(l.bias)\n",
    "            if not l.outLayer:\n",
    "              l.mDfaWeight = HE_Client.deserialize_matrix(l.mDfaWeight)\n",
    "\n",
    "    # test\n",
    "    def test(self, x_test, y_test):\n",
    "      # sample dimension first\n",
    "      samples = len(x_test)\n",
    "      corr = HE_Client.encode(0)\n",
    "      enc_x = HE_Client.encrypt_matrix(x_test)\n",
    "      enc_y = HE_Client.encrypt_matrix(y_test)\n",
    "\n",
    "      for j in range(samples):\n",
    "          # forward propagation\n",
    "          pred = self.predict(enc_x[j])\n",
    "          corr = TFHEValue(HE_Client.vm.gate_mux(pred == enc_y[j][0], (corr + 1).value, corr.value), corr.vm, corr.n_bits)\n",
    "\n",
    "      return corr\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        return encrypted_argmax(output[0])\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, miniBatchSize, lrInv):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            epochNumCorrect = 0\n",
    "            numIter = int(samples/miniBatchSize)\n",
    "\n",
    "            for j in range(numIter):\n",
    "                batchNumCorrect = 0\n",
    "                idxStart = j * miniBatchSize\n",
    "                idxEnd = idxStart + miniBatchSize\n",
    "\n",
    "                miniBatchImages = HE_Client.encrypt_matrix(x_train[idxStart:idxEnd])\n",
    "                miniBarchTargets = HE_Client.encrypt_matrix(y_train[idxStart:idxEnd])\n",
    "\n",
    "                # forward propagation\n",
    "                output = miniBatchImages\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                for layer in self.layers:\n",
    "                  output = layer.forward(output)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"End forward batch: \" + repr(j))\n",
    "                print(\"Computation time: \")\n",
    "                hours, rem = divmod(end_time-start_time, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "                print(\"\")\n",
    "\n",
    "                # sumLoss += batchL2Loss(miniBarchTargets, output);\n",
    "                lossDeltaMat = batchL2LossDelta(miniBarchTargets, output)\n",
    "\n",
    "                # dec_out = HE_Client.decrypt_matrix(output)\n",
    "                # dec_targets = HE_Client.decrypt_matrix(miniBarchTargets)\n",
    "\n",
    "                # for r in range(miniBatchSize):\n",
    "                #     if dec_targets[r].argmax() == dec_out[r].argmax():\n",
    "                #         batchNumCorrect += 1\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                # backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(lossDeltaMat, lrInv)\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"End backward batch: \" + repr(j))\n",
    "                print(\"Computation time: \")\n",
    "                hours, rem = divmod(end_time-start_time, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "                print(\"\")\n",
    "\n",
    "                epochNumCorrect += batchNumCorrect;\n",
    "\n",
    "            print(\"Epoch: \" + repr(i))\n",
    "            # print(\"EpochNumCorrect: \" + repr(epochNumCorrect))\n",
    "            # print(\"Accuracy: \" + repr(epochNumCorrect/samples * 100) + \" %\")\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encnet1_corr.pkl\", \"rb\") as f:\n",
    "  readNet1 = pickle.load(f)\n",
    "  readCorr1 = HE_Client.deserialize(pickle.load(f))\n",
    "  readNet1.deserialize()\n",
    "  print(HE_Client.decrypt(readCorr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encnet2_corr.pkl\", \"rb\") as f:\n",
    "  readNet2 = pickle.load(f)\n",
    "  readCorr2 = HE_Client.deserialize(pickle.load(f))\n",
    "  readNet2.deserialize()\n",
    "  print(HE_Client.decrypt(readCorr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encnet3_corr.pkl\", \"rb\") as f:\n",
    "  readNet3 = pickle.load(f)\n",
    "  readCorr3 = HE_Client.deserialize(pickle.load(f))\n",
    "  readNet3.deserialize()\n",
    "  print(HE_Client.decrypt(readCorr3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encnet4_corr.pkl\", \"rb\") as f:\n",
    "  readNet4 = pickle.load(f)\n",
    "  readCorr4 = HE_Client.deserialize(pickle.load(f))\n",
    "  readNet4.deserialize()\n",
    "  print(HE_Client.decrypt(readCorr4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = encrypted_argmax([readCorr1, readCorr2, readCorr3, readCorr4])\n",
    "print(HE_Client.decrypt(argmax))\n",
    "\n",
    "res = [HE_Client.encode(0), HE_Client.encode(0), HE_Client.encode(0), HE_Client.encode(0)]\n",
    "\n",
    "for i in range(len(res)):\n",
    "  enc_i = HE_Client.encode(i)\n",
    "  res[i] = TFHEValue(HE_Client.vm.gate_mux(enc_i == argmax, HE_Client.encode(1).value, res[i].value), res[i].vm, res[i].n_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = [readNet1.layers[0].weights, readNet2.layers[0].weights, readNet3.layers[0].weights, readNet4.layers[0].weights]\n",
    "bias1 = [readNet1.layers[0].bias, readNet2.layers[0].bias, readNet3.layers[0].bias, readNet4.layers[0].bias]\n",
    "\n",
    "weights2 = [readNet1.layers[1].weights, readNet2.layers[1].weights, readNet3.layers[1].weights, readNet4.layers[1].weights]\n",
    "bias2 = [readNet1.layers[1].bias, readNet2.layers[1].bias, readNet3.layers[1].bias, readNet4.layers[1].bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net = Network()\n",
    "res_net.add(FCLayer(4, 2))\n",
    "res_net.add(FCLayer(2, 3, outLayer=True))\n",
    "\n",
    "res_weights1 = np.full(weights1[0].shape, HE_Client.encode(0))\n",
    "res_bias1 = np.full(bias1[0].shape, HE_Client.encode(0))\n",
    "\n",
    "res_weights2 = np.full(weights2[0].shape, HE_Client.encode(0))\n",
    "res_bias2 = np.full(bias2[0].shape, HE_Client.encode(0))\n",
    "\n",
    "for i in range(len(res)):\n",
    "  res_weights1 = encrypted_mux_matrix(res[i] == HE_Client.encode(1), weights1[i], res_weights1)\n",
    "  res_bias1 = encrypted_mux_matrix(res[i] == HE_Client.encode(1), bias1[i], res_bias1)\n",
    "  res_weights2 = encrypted_mux_matrix(res[i] == HE_Client.encode(1), weights2[i], res_weights2)\n",
    "  res_bias2 = encrypted_mux_matrix(res[i] == HE_Client.encode(1), bias2[i], res_bias2)\n",
    "\n",
    "res_net.layers[0].weights = res_weights1\n",
    "res_net.layers[0].bias = res_bias1\n",
    "res_net.layers[1].weights = res_weights2\n",
    "res_net.layers[1].bias = res_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer 0:\")\n",
    "print(HE_Client.decrypt_matrix(res_net.layers[0].weights))\n",
    "print(HE_Client.decrypt_matrix(res_net.layers[0].bias))\n",
    "\n",
    "print(\"Layer 1:\")\n",
    "print(HE_Client.decrypt_matrix(res_net.layers[1].weights))\n",
    "print(HE_Client.decrypt_matrix(res_net.layers[1].bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layer 0:\")\n",
    "print(HE_Client.decrypt_matrix(readNet1.layers[0].weights))\n",
    "print(HE_Client.decrypt_matrix(readNet1.layers[0].bias))\n",
    "\n",
    "print(\"Layer 1:\")\n",
    "print(HE_Client.decrypt_matrix(readNet1.layers[1].weights))\n",
    "print(HE_Client.decrypt_matrix(readNet1.layers[1].bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_net = Network()\n",
    "plain_net.add(FCLayer(4, 2))\n",
    "plain_net.add(FCLayer(2, 3, outLayer=True))\n",
    "\n",
    "plain_net.layers[0].weights = HE_Client.decrypt_matrix(res_net.layers[0].weights)\n",
    "plain_net.layers[0].bias = HE_Client.decrypt_matrix(res_net.layers[0].bias)\n",
    "plain_net.layers[1].weights = HE_Client.decrypt_matrix(res_net.layers[1].weights)\n",
    "plain_net.layers[1].bias = HE_Client.decrypt_matrix(res_net.layers[1].bias)\n",
    "\n",
    "acc = plain_net.test(x_test, y_test)\n",
    "print(\"Testing accuracy best net \" + repr(HE_Client.decrypt(argmax)) + \": \" + repr(acc) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = []\n",
    "for net in [readNet1, readNet2, readNet3, readNet4]:\n",
    "  plain_net = Network()\n",
    "  plain_net.add(FCLayer(4, 2))\n",
    "  plain_net.add(FCLayer(2, 3, outLayer=True))\n",
    "\n",
    "  plain_net.layers[0].weights = HE_Client.decrypt_matrix(net.layers[0].weights)\n",
    "  plain_net.layers[0].bias = HE_Client.decrypt_matrix(net.layers[0].bias)\n",
    "  plain_net.layers[1].weights = HE_Client.decrypt_matrix(net.layers[1].weights)\n",
    "  plain_net.layers[1].bias = HE_Client.decrypt_matrix(net.layers[1].bias)\n",
    "\n",
    "  normNet = 0\n",
    "  for l in plain_net.layers:\n",
    "    normNet += np.linalg.norm(l.weights)\n",
    "    normNet += np.linalg.norm(l.bias)\n",
    "  \n",
    "  norms.append(normNet)\n",
    "  print(\"Norm net: \" + str(normNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_net = Network()\n",
    "plain_net.add(FCLayer(4, 2))\n",
    "plain_net.add(FCLayer(2, 3, outLayer=True))\n",
    "\n",
    "plain_net.layers[0].weights = HE_Client.decrypt_matrix(res_net.layers[0].weights)\n",
    "plain_net.layers[0].bias = HE_Client.decrypt_matrix(res_net.layers[0].bias)\n",
    "plain_net.layers[1].weights = HE_Client.decrypt_matrix(res_net.layers[1].weights)\n",
    "plain_net.layers[1].bias = HE_Client.decrypt_matrix(res_net.layers[1].bias)\n",
    "\n",
    "normResNet = 0\n",
    "for l in plain_net.layers:\n",
    "  normResNet += np.linalg.norm(l.weights)\n",
    "  normResNet += np.linalg.norm(l.bias)\n",
    "\n",
    "print(\"Norm res net: \" + str(normResNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(norms)):\n",
    "  print(\"||theta_\" + str(i+1) + \" - theta_beta||_2 = \" + str(norms[i]-normResNet))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
