{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pycrcnn.he.he import TFHEnuFHE\n",
    "from pycrcnn.he.tfhe_value import TFHEValue\n",
    "from pycrcnn.he.alu import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max quantization function\n",
    "def quantize_tensor(x, num_bits, min_val=None, max_val=None):\n",
    "    if not min_val and not max_val: \n",
    "        min_val, max_val = x.min(), x.max()\n",
    "    qmin = -2.**(num_bits-1)\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "   \n",
    "    x = x - min_val          # Allineo tutto l'array in modo che parta da 0\n",
    "    x /= (max_val - min_val) # Lo scalo tra 0 e 1    \n",
    "    x *= (qmax - qmin)       # Lo scalo tra 0 e 16\n",
    "    x -= qmax                # Lo sfaso tra -8 e 7\n",
    "    q_x = x.astype(float).round().astype(int)\n",
    "    \n",
    "    return q_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Penguins dataset\n",
    "penguins = pd.read_csv('../penguins_size.csv')\n",
    "penguins = penguins.sample(frac=1, random_state=2)\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "# Feature selection\n",
    "x_train, y_train = penguins.loc[:, [\"island\", \"culmen_length_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values, penguins.iloc[:, :1].values\n",
    "\n",
    "# Encode labels\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i][0] == \"Adelie\":\n",
    "        y_train[i][0] = 0\n",
    "    elif y_train[i][0] == \"Gentoo\":\n",
    "        y_train[i][0] = 1\n",
    "    else:\n",
    "        y_train[i][0] = 2\n",
    "\n",
    "island = {}\n",
    "countI = 0\n",
    "for i in range(len(x_train)):\n",
    "  # Island\n",
    "  if x_train[i][0] in island:\n",
    "      x_train[i][0] = island[x_train[i][0]]\n",
    "  else:\n",
    "      island[x_train[i][0]] = countI\n",
    "      x_train[i][0] = countI\n",
    "      countI += 1\n",
    "\n",
    "# Quantize tensors with 4 bits\n",
    "x_train[:, 1] = quantize_tensor(x_train[:, 1], 4)\n",
    "x_train[:, 2] = quantize_tensor(x_train[:, 2], 4)\n",
    "x_train[:, 3] = quantize_tensor(x_train[:, 3], 4)\n",
    "\n",
    "# Split Train-Validation set\n",
    "train, val = 150, 64\n",
    "x_val, y_val = x_train[train:train+val], y_train[train:train+val]\n",
    "x_test, y_test = x_train[train+val:], y_train[train+val:]\n",
    "y_train = np_utils.to_categorical(y_train).astype(int)*8\n",
    "x_train, y_train = x_train[:train], y_train[:train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_weights_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Loss Function\n",
    "def L2(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual matmul used to check overflow\n",
    "def matmul(m1, m2, check_overflow=False, bits_tfhe = 0, clip_overflow=False, layer=\"\"):\n",
    "    max_value = 2.**(bits_tfhe-1) - 1.\n",
    "    min_value = -2.**(bits_tfhe-1)\n",
    "    res = [[0 for i in range(len(m2[0]))] for j in range(len(m1))]\n",
    "\n",
    "    for i in range(len(m1)):\n",
    "        for j in range(len(m2[0])):\n",
    "            for k in range(len(m2)):\n",
    "                mul = m1[i][k] * m2[k][j]\n",
    "\n",
    "                if clip_overflow:\n",
    "                    mul = max_value if mul>max_value else mul\n",
    "                    mul = min_value if mul<min_value else mul\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if mul>max_value else 0\n",
    "                    overflow += 1 if mul<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"MUL: matmul overflow layer: \" + layer)\n",
    "\n",
    "                res[i][j] += mul\n",
    "\n",
    "                if clip_overflow:\n",
    "                    res[i][j] = max_value if res[i][j]>max_value else res[i][j]\n",
    "                    res[i][j] = min_value if res[i][j]<min_value else res[i][j]\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if res[i][j]>max_value else 0\n",
    "                    overflow += 1 if res[i][j]<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"ADD: matmul overflow layer: \" + layer)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool Layer\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, kernel_size, stride=(1, 1)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, batch, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return np.array([_max(image, self.kernel_size, self.stride) for image in batch])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss\n",
    "\n",
    "def _max(image, kernel_size, stride):\n",
    "    x_s = stride[1]\n",
    "    y_s = stride[0]\n",
    "\n",
    "    x_k = kernel_size[1]\n",
    "    y_k = kernel_size[0]\n",
    "\n",
    "    # print(image)\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "\n",
    "    x_o = ((x_d - x_k) // x_s) + 1\n",
    "    y_o = ((y_d - y_k) // y_s) + 1\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_s\n",
    "        index_column = x * x_s\n",
    "        return matrix[index_row: index_row + y_k, index_column: index_column + x_k]\n",
    "\n",
    "    return [[np.max(get_submatrix(image, x, y).flatten()) for x in range(0, x_o)] for y in range(0, y_o)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, flatten_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return flatten_in.reshape(flatten_in.shape[0], flatten_in.shape[1]*flatten_in.shape[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"fw input=\"  + repr(self.in_dim)\n",
    "        self.input = fc_in\n",
    "\n",
    "        if check_overflow or clip_overflow:\n",
    "            dot = matmul(self.input, self.weights, check_overflow, bits_tfhe, clip_overflow, layer) + self.bias\n",
    "        else:\n",
    "            dot = (self.input @ self.weights) + self.bias\n",
    "\n",
    "        if clip_overflow:\n",
    "            dot = np.clip(dot, min_value, max_value)\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (dot[dot>max_value]).size\n",
    "            overflow += (dot[dot<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: Bias overflow layer: \"  + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):   \n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"bw input=\"  + repr(self.in_dim)\n",
    "\n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (d_DFA[d_DFA>max_value]).size\n",
    "            overflow += (d_DFA[d_DFA<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Deltas overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        if check_overflow or clip_overflow:\n",
    "            weights_update = matmul(self.input.T, d_DFA, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            weights_update = self.input.T @ d_DFA\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (weights_update[weights_update>max_value]).size\n",
    "            overflow += (weights_update[weights_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Weights Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.weights[self.weights>max_value]).size\n",
    "            overflow += (self.weights[self.weights<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: weights overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        if check_overflow or clip_overflow:\n",
    "            bias_update = matmul(d_DFA.T, ones, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            bias_update = d_DFA.T @ ones\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (bias_update[bias_update>max_value]).size\n",
    "            overflow += (bias_update[bias_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Bias Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.bias[self.bias>max_value]).size\n",
    "            overflow += (self.bias[self.bias<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: bias overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False, layer=\"\"):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            if check_overflow or clip_overflow:\n",
    "                dot = matmul(loss, self.DFA_weights, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "            else:\n",
    "                dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j], check_overflow, bits_tfhe, clip_overflow)\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, check_overflow, bits_tfhe, clip_overflow)\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, mini_batch_size, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        train_accs, val_accs = [], []\n",
    "        for i in range(epochs):\n",
    "            epoch_corr = 0\n",
    "            for j in range(int(len(x_train)/mini_batch_size)):\n",
    "                batch_corr = 0\n",
    "                idx_start = j * mini_batch_size\n",
    "                idx_end = idx_start + mini_batch_size\n",
    "\n",
    "                batch_in = x_train[idx_start:idx_end]\n",
    "                batch_target = y_train[idx_start:idx_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                for layer in self.layers:\n",
    "                  batch_in = layer.forward(batch_in, check_overflow, bits_tfhe, clip_overflow)\n",
    "                fwd_out = batch_in               \n",
    "\n",
    "                # Loss\n",
    "                loss = L2(batch_target, fwd_out)\n",
    "\n",
    "                for r in range(mini_batch_size):\n",
    "                    if batch_target[r].argmax() == fwd_out[r].argmax():\n",
    "                        batch_corr += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow)\n",
    "                \n",
    "                epoch_corr += batch_corr\n",
    "\n",
    "            acc = epoch_corr/len(x_train) * 100\n",
    "            train_accs.append(acc)\n",
    "            val_accs.append(self.test(x_val, y_val))\n",
    "            \n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPLOADE DFA WEIGHTS\n",
    "DFA_weights = np.load(\"res/DFA_weights1.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted trained weights TFHE-NN-1\n",
    "with open(\"res/trained_weights1.pkl\", \"rb\") as f:\n",
    "    decrypted_weights1_1 = pickle.load(f)\n",
    "    decrypted_bias1_1 = pickle.load(f)\n",
    "    decrypted_weights2_1 = pickle.load(f)\n",
    "    decrypted_bias2_1 = pickle.load(f)\n",
    "    decrypted_corr1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted validation accuracy TFHE-NN-1: 92.1875 %\n"
     ]
    }
   ],
   "source": [
    "# Trained TFHE-NN-1 with decryted trained weights on encrypted data\n",
    "trained_net1 = Network()\n",
    "trained_net1.add(FCLayer(4, 2))\n",
    "trained_net1.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "trained_net1.layers[0].DFA_weights = DFA_weights\n",
    "\n",
    "trained_net1.layers[0].weights = decrypted_weights1_1\n",
    "trained_net1.layers[0].bias = decrypted_bias1_1\n",
    "trained_net1.layers[1].weights = decrypted_weights2_1\n",
    "trained_net1.layers[1].bias = decrypted_bias2_1\n",
    "\n",
    "print(\"Decrypted validation accuracy TFHE-NN-1: \" + repr(decrypted_corr1*100/len(x_val)) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted trained weights TFHE-NN-2\n",
    "with open(\"res/trained_weights2.pkl\", \"rb\") as f:\n",
    "    decrypted_weights1_2 = pickle.load(f)\n",
    "    decrypted_bias1_2 = pickle.load(f)\n",
    "    decrypted_weights2_2 = pickle.load(f)\n",
    "    decrypted_bias2_2 = pickle.load(f)\n",
    "    decrypted_corr2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted validation accuracy TFHE-NN-2: 87.5 %\n"
     ]
    }
   ],
   "source": [
    "# Trained TFHE-NN-2 with decryted trained weights on encrypted data\n",
    "trained_net2 = Network()\n",
    "trained_net2.add(FCLayer(4, 2))\n",
    "trained_net2.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "trained_net2.layers[0].DFA_weights = DFA_weights\n",
    "\n",
    "trained_net2.layers[0].weights = decrypted_weights1_2\n",
    "trained_net2.layers[0].bias = decrypted_bias1_2\n",
    "trained_net2.layers[1].weights = decrypted_weights2_2\n",
    "trained_net2.layers[1].bias = decrypted_bias2_2\n",
    "\n",
    "print(\"Decrypted validation accuracy TFHE-NN-2: \" + repr(decrypted_corr2*100/len(x_val)) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted trained weights TFHE-NN-3\n",
    "with open(\"res/trained_weights3.pkl\", \"rb\") as f:\n",
    "    decrypted_weights1_3 = pickle.load(f)\n",
    "    decrypted_bias1_3 = pickle.load(f)\n",
    "    decrypted_weights2_3 = pickle.load(f)\n",
    "    decrypted_bias2_3 = pickle.load(f)\n",
    "    decrypted_corr3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted validation accuracy TFHE-NN-3: 85.9375 %\n"
     ]
    }
   ],
   "source": [
    "# Trained TFHE-NN-3 with decryted trained weights on encrypted data\n",
    "trained_net3 = Network()\n",
    "trained_net3.add(FCLayer(4, 2))\n",
    "trained_net3.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "trained_net3.layers[0].DFA_weights = DFA_weights\n",
    "\n",
    "trained_net3.layers[0].weights = decrypted_weights1_3\n",
    "trained_net3.layers[0].bias = decrypted_bias1_3\n",
    "trained_net3.layers[1].weights = decrypted_weights2_3\n",
    "trained_net3.layers[1].bias = decrypted_bias2_3\n",
    "\n",
    "print(\"Decrypted validation accuracy TFHE-NN-3: \" + repr(decrypted_corr3*100/len(x_val)) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted trained weights TFHE-NN-4\n",
    "with open(\"res/trained_weights4.pkl\", \"rb\") as f:\n",
    "    decrypted_weights1_4 = pickle.load(f)\n",
    "    decrypted_bias1_4 = pickle.load(f)\n",
    "    decrypted_weights2_4 = pickle.load(f)\n",
    "    decrypted_bias2_4 = pickle.load(f)\n",
    "    decrypted_corr4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted validation accuracy TFHE-NN-4: 84.375 %\n"
     ]
    }
   ],
   "source": [
    "# Trained TFHE-NN-4 with decryted trained weights on encrypted data\n",
    "trained_net4 = Network()\n",
    "trained_net4.add(FCLayer(4, 2))\n",
    "trained_net4.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "trained_net4.layers[0].DFA_weights = DFA_weights\n",
    "\n",
    "trained_net4.layers[0].weights = decrypted_weights1_4\n",
    "trained_net4.layers[0].bias = decrypted_bias1_4\n",
    "trained_net4.layers[1].weights = decrypted_weights2_4\n",
    "trained_net4.layers[1].bias = decrypted_bias2_4\n",
    "\n",
    "print(\"Decrypted validation accuracy TFHE-NN-4: \" + repr(decrypted_corr4*100/len(x_val)) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Resulting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted cross validated weights\n",
    "with open(\"res/cross_validated_weights.pkl\", \"rb\") as f:\n",
    "    decrypted_weights1_cv = pickle.load(f)\n",
    "    decrypted_bias1_cv = pickle.load(f)\n",
    "    decrypted_weights2_cv = pickle.load(f)\n",
    "    decrypted_bias2_cv = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulting CV TFHE-NN with decryted cross validated weights\n",
    "cv_net = Network()\n",
    "cv_net.add(FCLayer(4, 2))\n",
    "cv_net.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "cv_net.layers[0].DFA_weights = DFA_weights\n",
    "\n",
    "cv_net.layers[0].weights = decrypted_weights1_cv\n",
    "cv_net.layers[0].bias = decrypted_bias1_cv\n",
    "cv_net.layers[1].weights = decrypted_weights2_cv\n",
    "cv_net.layers[1].bias = decrypted_bias2_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy CV TFHE-NN: 92.1875 %\n"
     ]
    }
   ],
   "source": [
    "val_acc_cv = cv_net.test(x_val, y_val, check_overflow=True, bits_tfhe=16)\n",
    "print(\"Validation accuracy CV TFHE-NN: \" + repr(val_acc_cv) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy CV TFHE-NN: 93.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "test_acc_cv = cv_net.test(x_test, y_test, check_overflow=True, bits_tfhe=16)\n",
    "print(\"Test accuracy CV TFHE-NN: \" + repr(test_acc_cv) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Norms Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norm of learned weights of the 4 models on encrypted data\n",
    "norm_encrypted_weights, norm_plain_weights = 0, 0\n",
    "\n",
    "L2_norms = []\n",
    "for net in [trained_net1, trained_net2, trained_net3, trained_net4]:\n",
    "  norm = 0\n",
    "  for l in net.layers:\n",
    "      if hasattr(l, \"weights\"):\n",
    "          norm += np.linalg.norm(l.weights)\n",
    "          norm += np.linalg.norm(l.bias)\n",
    "  L2_norms.append(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norm of cross validated weights on encrypted data\n",
    "norm_cv_net = 0\n",
    "for l in cv_net.layers:\n",
    "    norm_cv_net += np.linalg.norm(l.weights)\n",
    "    norm_cv_net += np.linalg.norm(l.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference L2 norms TFHE-NN-1 vs. CV TFHE-NN: 0.0\n",
      "Difference L2 norms TFHE-NN-2 vs. CV TFHE-NN: -414.5183905686772\n",
      "Difference L2 norms TFHE-NN-3 vs. CV TFHE-NN: -1112.5233572468514\n",
      "Difference L2 norms TFHE-NN-4 vs. CV TFHE-NN: -2595.853748717702\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(L2_norms)):\n",
    "    print(\"Difference L2 norms TFHE-NN-\" + str(i+1) + \" vs. CV TFHE-NN: \" + str(L2_norms[i] - norm_cv_net))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
