{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-21 13:57:24.899842: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-21 13:57:25.491307: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-21 13:57:27.005837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-21 13:57:27.005948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-01-21 13:57:27.005960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from pycrcnn.he.HE import TFHEnuFHE\n",
    "from pycrcnn.he.tfhe_value import TFHEValue\n",
    "from pycrcnn.he.alu import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TernaryMNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Train set\n",
    "x_train = x_train[:, 6:22, 6:22]\n",
    "\n",
    "# Create Ternary classification dataset\n",
    "train_indexes, test_indexes = [], []\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i] == 0 or y_train[i] == 1 or y_train[i] == 2:\n",
    "        train_indexes.append(i)\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] == 0 or y_test[i] == 1 or y_test[i] == 2:\n",
    "        test_indexes.append(i)\n",
    "x_train = np.subtract(x_train[train_indexes], 128)\n",
    "x_train.dtype = np.int8\n",
    "y_train = y_train[train_indexes]\n",
    "\n",
    "val_images = 5000\n",
    "idx_train = len(x_train) - val_images\n",
    "x_train, x_val = x_train[:idx_train], x_train[idx_train:]\n",
    "y_train, y_val = y_train[:idx_train], y_train[idx_train:]\n",
    "y_train = np_utils.to_categorical(y_train).astype(int)*16\n",
    "\n",
    "# Test set\n",
    "x_test = x_test[:, 6:22, 6:22]\n",
    "x_test = np.subtract(x_test[test_indexes], 128)\n",
    "x_test.dtype = np.int8\n",
    "y_test = y_test[test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_weights_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Loss Function\n",
    "def L2(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual matmul used to check overflow\n",
    "def matmul(m1, m2, check_overflow=False, bits_tfhe = 0, clip_overflow=False, layer=\"\"):\n",
    "    max_value = 2.**(bits_tfhe-1) - 1.\n",
    "    min_value = -2.**(bits_tfhe-1)\n",
    "    res = [[0 for i in range(len(m2[0]))] for j in range(len(m1))]\n",
    "\n",
    "    for i in range(len(m1)):\n",
    "        for j in range(len(m2[0])):\n",
    "            for k in range(len(m2)):\n",
    "                mul = m1[i][k] * m2[k][j]\n",
    "\n",
    "                if clip_overflow:\n",
    "                    mul = max_value if mul>max_value else mul\n",
    "                    mul = min_value if mul<min_value else mul\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if mul>max_value else 0\n",
    "                    overflow += 1 if mul<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"MUL: matmul overflow layer: \" + layer)\n",
    "\n",
    "                res[i][j] += mul\n",
    "\n",
    "                if clip_overflow:\n",
    "                    res[i][j] = max_value if res[i][j]>max_value else res[i][j]\n",
    "                    res[i][j] = min_value if res[i][j]<min_value else res[i][j]\n",
    "\n",
    "                if check_overflow:\n",
    "                    overflow = 1 if res[i][j]>max_value else 0\n",
    "                    overflow += 1 if res[i][j]<min_value else 0\n",
    "                    if overflow > 0:\n",
    "                        print(\"ADD: matmul overflow layer: \" + layer)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPool Layer\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, kernel_size, stride=(1, 1)):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, batch, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return np.array([_max(image, self.kernel_size, self.stride) for image in batch])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss\n",
    "\n",
    "def _max(image, kernel_size, stride):\n",
    "    x_s = stride[1]\n",
    "    y_s = stride[0]\n",
    "\n",
    "    x_k = kernel_size[1]\n",
    "    y_k = kernel_size[0]\n",
    "\n",
    "    # print(image)\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "\n",
    "    x_o = ((x_d - x_k) // x_s) + 1\n",
    "    y_o = ((y_d - y_k) // y_s) + 1\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_s\n",
    "        index_column = x * x_s\n",
    "        return matrix[index_row: index_row + y_k, index_column: index_column + x_k]\n",
    "\n",
    "    return [[np.max(get_submatrix(image, x, y).flatten()) for x in range(0, x_o)] for y in range(0, y_o)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, flatten_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return flatten_in.reshape(flatten_in.shape[0], flatten_in.shape[1]*flatten_in.shape[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in, check_overflow=False, bits_tfhe = 0, clip_overflow=False):\n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"fw input=\"  + repr(self.in_dim)\n",
    "        self.input = fc_in\n",
    "\n",
    "        if check_overflow or clip_overflow:\n",
    "            dot = matmul(self.input, self.weights, check_overflow, bits_tfhe, clip_overflow, layer) + self.bias\n",
    "        else:\n",
    "            dot = (self.input @ self.weights) + self.bias\n",
    "\n",
    "        if clip_overflow:\n",
    "            dot = np.clip(dot, min_value, max_value)\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (dot[dot>max_value]).size\n",
    "            overflow += (dot[dot<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: Bias overflow layer: \"  + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):   \n",
    "        max_value = 2.**(bits_tfhe-1) - 1.\n",
    "        min_value = -2.**(bits_tfhe-1)\n",
    "        layer = \"bw input=\"  + repr(self.in_dim)\n",
    "\n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (d_DFA[d_DFA>max_value]).size\n",
    "            overflow += (d_DFA[d_DFA<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Deltas overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        if check_overflow or clip_overflow:\n",
    "            weights_update = matmul(self.input.T, d_DFA, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            weights_update = self.input.T @ d_DFA\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (weights_update[weights_update>max_value]).size\n",
    "            overflow += (weights_update[weights_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Weights Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.weights[self.weights>max_value]).size\n",
    "            overflow += (self.weights[self.weights<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: weights overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        if check_overflow or clip_overflow:\n",
    "            bias_update = matmul(d_DFA.T, ones, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "        else:\n",
    "            bias_update = d_DFA.T @ ones\n",
    "        \n",
    "        if check_overflow:\n",
    "            overflow = (bias_update[bias_update>max_value]).size\n",
    "            overflow += (bias_update[bias_update<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"Bias Update overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update\n",
    "\n",
    "        if check_overflow:\n",
    "            overflow = (self.bias[self.bias>max_value]).size\n",
    "            overflow += (self.bias[self.bias<min_value]).size\n",
    "            if overflow > 0:\n",
    "                print(\"ADD: bias overflow layer: \" + layer)\n",
    "                print(overflow)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False, layer=\"\"):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            if check_overflow or clip_overflow:\n",
    "                dot = matmul(loss, self.DFA_weights, check_overflow, bits_tfhe, clip_overflow, layer)\n",
    "            else:\n",
    "                dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j], check_overflow, bits_tfhe, clip_overflow)\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, check_overflow, bits_tfhe, clip_overflow)\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, batch_size, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        train_accs, val_accs = [], []\n",
    "        for i in range(epochs):\n",
    "            epoch_corr = 0\n",
    "            for j in range(int(len(x_train)/batch_size)):\n",
    "                batch_corr = 0\n",
    "                idx_start = j * batch_size\n",
    "                idx_end = idx_start + batch_size\n",
    "\n",
    "                batch_in = x_train[idx_start:idx_end]\n",
    "                batch_target = y_train[idx_start:idx_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                for layer in self.layers:\n",
    "                  batch_in = layer.forward(batch_in, check_overflow, bits_tfhe, clip_overflow)\n",
    "                fwd_out = batch_in               \n",
    "\n",
    "                # Loss\n",
    "                loss = L2(batch_target, fwd_out)\n",
    "\n",
    "                for r in range(batch_size):\n",
    "                    if batch_target[r].argmax() == fwd_out[r].argmax():\n",
    "                        batch_corr += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow)\n",
    "                \n",
    "                epoch_corr += batch_corr\n",
    "\n",
    "            acc = epoch_corr/len(x_train) * 100\n",
    "            train_accs.append(acc)\n",
    "            val_accs.append(self.test(x_val, y_val))\n",
    "            \n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPLOAD DFA WEIGHTS\n",
    "DFA_weights1 = np.load(\"DFAWeights_L1.npy\")\n",
    "DFA_weights2 = np.load(\"DFAWeights_L2.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encrypted Version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plain_weights_M1.pkl\", \"rb\") as f:\n",
    "    weights2M1 = pickle.load(f)\n",
    "    bias2M1 = pickle.load(f)\n",
    "    weights3M1 = pickle.load(f)\n",
    "    bias3M1 = pickle.load(f)\n",
    "    weights4M1 = pickle.load(f)\n",
    "    bias4M1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net1 = Network()\n",
    "net1.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "net1.add(FlattenLayer())\n",
    "net1.add(FCLayer(16, 4))\n",
    "net1.add(FCLayer(4, 2))\n",
    "net1.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "net1.layers[2].DFA_weights = DFA_weights1\n",
    "net1.layers[3].DFA_weights = DFA_weights2\n",
    "\n",
    "net1.layers[2].weights = weights2M1\n",
    "net1.layers[2].bias = bias2M1\n",
    "net1.layers[3].weights = weights3M1\n",
    "net1.layers[3].bias = bias3M1\n",
    "net1.layers[4].weights = weights4M1\n",
    "net1.layers[4].bias = bias4M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 77.3 %\n"
     ]
    }
   ],
   "source": [
    "acc = net1.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Val accuracy: \" + repr(acc) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plain_weights_M2.pkl\", \"rb\") as f:\n",
    "    weights2M2 = pickle.load(f)\n",
    "    bias2M2 = pickle.load(f)\n",
    "    weights3M2 = pickle.load(f)\n",
    "    bias3M2 = pickle.load(f)\n",
    "    weights4M2 = pickle.load(f)\n",
    "    bias4M2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net2 = Network()\n",
    "net2.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "net2.add(FlattenLayer())\n",
    "net2.add(FCLayer(16, 4))\n",
    "net2.add(FCLayer(4, 2))\n",
    "net2.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "net2.layers[2].DFA_weights = DFA_weights1\n",
    "net2.layers[3].DFA_weights = DFA_weights2\n",
    "\n",
    "net2.layers[2].weights = weights2M2\n",
    "net2.layers[2].bias = bias2M2\n",
    "net2.layers[3].weights = weights3M2\n",
    "net2.layers[3].bias = bias3M2\n",
    "net2.layers[4].weights = weights4M2\n",
    "net2.layers[4].bias = bias4M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 70.12 %\n"
     ]
    }
   ],
   "source": [
    "acc = net2.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Val accuracy: \" + repr(acc) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plain_weights_Aggr.pkl\", \"rb\") as f:\n",
    "    weights2Aggr = pickle.load(f)\n",
    "    bias2Aggr = pickle.load(f)\n",
    "    weights3Aggr = pickle.load(f)\n",
    "    bias3Aggr = pickle.load(f)\n",
    "    weights4Aggr = pickle.load(f)\n",
    "    bias4Aggr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "netAggr = Network()\n",
    "netAggr.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "netAggr.add(FlattenLayer())\n",
    "netAggr.add(FCLayer(16, 4))\n",
    "netAggr.add(FCLayer(4, 2))\n",
    "netAggr.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "netAggr.layers[2].DFA_weights = DFA_weights1\n",
    "netAggr.layers[3].DFA_weights = DFA_weights2\n",
    "\n",
    "netAggr.layers[2].weights = weights2Aggr\n",
    "netAggr.layers[2].bias = bias2Aggr\n",
    "netAggr.layers[3].weights = weights3Aggr\n",
    "netAggr.layers[3].bias = bias3Aggr\n",
    "netAggr.layers[4].weights = weights4Aggr\n",
    "netAggr.layers[4].bias = bias4Aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 86.48 %\n"
     ]
    }
   ],
   "source": [
    "acc = netAggr.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Val accuracy: \" + repr(acc) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.68573244359708 %\n"
     ]
    }
   ],
   "source": [
    "acc = netAggr.test(x_test, y_test, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Test accuracy: \" + repr(acc) + \" %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy net1: 77.3 % LR: 256 BS: 5\n",
      "Validation accuracy net2: 70.12 % LR: 256 BS: 5\n"
     ]
    }
   ],
   "source": [
    "# Net 1 structure\n",
    "net1 = Network()\n",
    "net1.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "net1.add(FlattenLayer())\n",
    "net1.add(FCLayer(16, 4))\n",
    "net1.add(FCLayer(4, 2))\n",
    "net1.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "net1.layers[2].DFA_weights = DFA_weights1\n",
    "net1.layers[3].DFA_weights = DFA_weights2\n",
    "\n",
    "# Net 2 structure\n",
    "net2 = Network()\n",
    "net2.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "net2.add(FlattenLayer())\n",
    "net2.add(FCLayer(16, 4))\n",
    "net2.add(FCLayer(4, 2))\n",
    "net2.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "net2.layers[2].DFA_weights = DFA_weights1\n",
    "net2.layers[3].DFA_weights = DFA_weights2\n",
    "\n",
    "# Train\n",
    "train_acc1, val_acc1 = net1.fit(x_train[100:125], y_train[100:125], epochs=3, batch_size=5, lr_inv=256, check_overflow=True, bits_tfhe=22, clip_overflow=False)\n",
    "train_acc2, val_acc2 = net2.fit(x_train[125:150], y_train[125:150], epochs=3, batch_size=5, lr_inv=256, check_overflow=True, bits_tfhe=22, clip_overflow=False)\n",
    "\n",
    "print(\"Validation accuracy net1: \" + repr(val_acc1[-1]) + \" %\" + \" LR: 256 BS: 5\")\n",
    "print(\"Validation accuracy net2: \" + repr(val_acc2[-1]) + \" %\" + \" LR: 256 BS: 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = [net1.layers[2].weights, net2.layers[2].weights]\n",
    "bias2 = [net1.layers[2].bias, net2.layers[2].bias]\n",
    "\n",
    "weights3 = [net1.layers[3].weights, net2.layers[3].weights]\n",
    "bias3 = [net1.layers[3].bias, net2.layers[3].bias]\n",
    "\n",
    "weights4 = [net1.layers[4].weights, net2.layers[4].weights]\n",
    "bias4 = [net1.layers[4].bias, net2.layers[4].bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average net accuracy: 86.48 %\n"
     ]
    }
   ],
   "source": [
    "# Net structure\n",
    "averageNet = Network()\n",
    "averageNet.add(MaxPoolLayer((4, 4), stride=(4, 4)))\n",
    "averageNet.add(FlattenLayer())\n",
    "averageNet.add(FCLayer(16, 4))\n",
    "averageNet.add(FCLayer(4, 2))\n",
    "averageNet.add(FCLayer(2, 3, last_layer=True))\n",
    "\n",
    "average_weights2 = np.mean(weights2, axis=0).astype(int)\n",
    "average_bias2 = np.mean(bias2, axis=0).astype(int)\n",
    "average_weights3 = np.mean(weights3, axis=0).astype(int)\n",
    "average_bias3 = np.mean(bias3, axis=0).astype(int)\n",
    "average_weights4 = np.mean(weights4, axis=0).astype(int)\n",
    "average_bias4 = np.mean(bias4, axis=0).astype(int)\n",
    "\n",
    "averageNet.layers[2].weights = average_weights2\n",
    "averageNet.layers[2].bias = average_bias2\n",
    "averageNet.layers[3].weights = average_weights3\n",
    "averageNet.layers[3].bias = average_bias3\n",
    "averageNet.layers[4].weights = average_weights4\n",
    "averageNet.layers[4].bias = average_bias4\n",
    "\n",
    "acc = averageNet.test(x_val, y_val, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Average net accuracy: \" + repr(acc) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.68573244359708 %\n"
     ]
    }
   ],
   "source": [
    "acc = averageNet.test(x_test, y_test, check_overflow=True, bits_tfhe=22)\n",
    "print(\"Test accuracy: \" + repr(acc) + \" %\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
