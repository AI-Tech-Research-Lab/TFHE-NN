{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.stats as ss\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare FashionMNIST Dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "# Train and Validation sets\n",
    "x_train, x_val = x_train[:50000], x_train[50000:]\n",
    "y_train, y_val = y_train[:50000], y_train[50000:]\n",
    "\n",
    "# Split the train dataset into different batches\n",
    "num_batches = 128\n",
    "images_per_batch = int(len(x_train)/num_batches)\n",
    "\n",
    "train_batches = [[0, 0] for i in range(num_batches)]\n",
    "\n",
    "for i in range(num_batches):\n",
    "    idx = images_per_batch*i\n",
    "    train_batches[i][0] = np.subtract(x_train[idx:idx+images_per_batch], 128)\n",
    "    train_batches[i][0].dtype = np.int8\n",
    "    train_batches[i][1] = np_utils.to_categorical(y_train[idx:idx+images_per_batch]).astype(int)*16\n",
    "\n",
    "# Validation set\n",
    "x_val = np.subtract(x_val, 128)\n",
    "x_val.dtype = np.int8\n",
    "\n",
    "# Test set\n",
    "x_test = np.subtract(x_test, 128)\n",
    "x_test.dtype = np.int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.714668Z",
     "iopub.status.busy": "2023-01-15T10:48:22.714426Z",
     "iopub.status.idle": "2023-01-15T10:48:22.720930Z",
     "shell.execute_reply": "2023-01-15T10:48:22.719390Z"
    }
   },
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.726927Z",
     "iopub.status.busy": "2023-01-15T10:48:22.726690Z",
     "iopub.status.idle": "2023-01-15T10:48:22.733055Z",
     "shell.execute_reply": "2023-01-15T10:48:22.731435Z"
    }
   },
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_weights_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4 + 88\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val + 32\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val - 32\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4 - 88\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.774551Z",
     "iopub.status.busy": "2023-01-15T10:48:22.774125Z",
     "iopub.status.idle": "2023-01-15T10:48:22.787663Z",
     "shell.execute_reply": "2023-01-15T10:48:22.785993Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2 Loss Function\n",
    "def L2(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, flatten_in):\n",
    "        return flatten_in.reshape(flatten_in.shape[0], flatten_in.shape[1]*flatten_in.shape[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.794879Z",
     "iopub.status.busy": "2023-01-15T10:48:22.794383Z",
     "iopub.status.idle": "2023-01-15T10:48:22.812688Z",
     "shell.execute_reply": "2023-01-15T10:48:22.810867Z"
    }
   },
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in):\n",
    "        self.input = fc_in\n",
    "        dot = (self.input @ self.weights) + self.bias       \n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv):   \n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv)\n",
    "        weights_update = self.input.T @ d_DFA      \n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        bias_update = d_DFA.T @ ones      \n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update        \n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False, layer=\"\"):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j], check_overflow, bits_tfhe, clip_overflow)\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output, check_overflow, bits_tfhe, clip_overflow)\n",
    "        return output.argmax()\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, mini_batch_size, lr_inv, check_overflow=False, bits_tfhe=0, clip_overflow=False):\n",
    "        train_accs, val_accs, weights = [], [], []\n",
    "        max_val_acc = 0\n",
    "        for i in range(epochs):\n",
    "            epoch_corr = 0\n",
    "            for j in range(int(len(x_train)/mini_batch_size)):\n",
    "                batch_corr = 0\n",
    "                idx_start = j * mini_batch_size\n",
    "                idx_end = idx_start + mini_batch_size\n",
    "\n",
    "                batch_in = x_train[idx_start:idx_end]\n",
    "                batch_target = y_train[idx_start:idx_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                for layer in self.layers:\n",
    "                  batch_in = layer.forward(batch_in, check_overflow, bits_tfhe, clip_overflow)\n",
    "                fwd_out = batch_in               \n",
    "\n",
    "                # Loss\n",
    "                loss = L2(batch_target, fwd_out)\n",
    "\n",
    "                for r in range(mini_batch_size):\n",
    "                    if batch_target[r].argmax() == fwd_out[r].argmax():\n",
    "                        batch_corr += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    layer.backward(loss, lr_inv, check_overflow, bits_tfhe, clip_overflow)\n",
    "                \n",
    "                epoch_corr += batch_corr\n",
    "\n",
    "            acc = epoch_corr/len(x_train) * 100\n",
    "            train_accs.append(acc)\n",
    "\n",
    "            # Validation accuracy\n",
    "            val_acc = self.test(x_val, y_val)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "            # Save weights of the best model\n",
    "            if len(val_accs) == 1 or val_accs[-1] > max_val_acc:\n",
    "                weights = [np.copy(self.layers[1].weights), np.copy(self.layers[1].bias), np.copy(self.layers[2].weights), np.copy(self.layers[2].bias)]\n",
    "                max_val_acc = val_acc\n",
    "        return train_accs, val_accs, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.887218Z",
     "iopub.status.busy": "2023-01-15T10:48:22.886699Z",
     "iopub.status.idle": "2023-01-15T10:48:22.894784Z",
     "shell.execute_reply": "2023-01-15T10:48:22.892957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters Configuration H\n",
    "bs = 50\n",
    "epochs = 20\n",
    "lrs_inv = [2048, 4096, 8192]\n",
    "\n",
    "# Number of runs\n",
    "runs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.902121Z",
     "iopub.status.busy": "2023-01-15T10:48:22.901534Z",
     "iopub.status.idle": "2023-01-15T10:48:22.911146Z",
     "shell.execute_reply": "2023-01-15T10:48:22.909473Z"
    }
   },
   "outputs": [],
   "source": [
    "## UPLOAD DFA WEIGHTS\n",
    "DFA_weights1 = np.load(\"res/DFAWeights1.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training M models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch, run, lr):\n",
    "    # Net structure\n",
    "    net = Network()\n",
    "    net.add(FlattenLayer())\n",
    "    net.add(FCLayer(28*28, 200))\n",
    "    net.add(FCLayer(200, 10, outLayer=True))\n",
    "\n",
    "    net.layers[1].mDfaWeight = DFA_weights1[run]\n",
    "\n",
    "    # Train\n",
    "    _, val_acc, weights = net.fit(train_batches[batch][0], train_batches[batch][1], epochs=epochs, mini_batch_size=bs, lr_inv=lr)\n",
    "  \n",
    "    return {str(lr)+'-'+str(run)+'-'+str(batch): [val_acc, weights]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.938438Z",
     "iopub.status.busy": "2023-01-15T10:48:22.937856Z",
     "iopub.status.idle": "2023-01-16T03:12:31.791541Z",
     "shell.execute_reply": "2023-01-16T03:12:31.789504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 59048.84340643883 s\n"
     ]
    }
   ],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "start_time = time.time()\n",
    "part_res = Parallel(n_jobs=-1)(delayed(train)(batch, run, lr) for lr in lrs_inv for run in range(runs) for batch in range(num_batches))\n",
    "end_time = time.time()\n",
    "print(\"Total time: \" + str(end_time-start_time) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:12:31.799870Z",
     "iopub.status.busy": "2023-01-16T03:12:31.799250Z",
     "iopub.status.idle": "2023-01-16T03:12:32.190313Z",
     "shell.execute_reply": "2023-01-16T03:12:32.188734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder results of networks trained on single batches of the dataset\n",
    "part_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs_mean, accs_max, W = [], [], []\n",
    "    for run in range(runs):\n",
    "        for batch in range(num_batches):\n",
    "            index, dict_key = runs*num_batches*lr_idx+num_batches*run+batch, str(lrs_inv[lr_idx])+'-'+str(run)+'-'+str(batch)\n",
    "            accs_mean.append(np.mean(part_res[index][dict_key][0])/100)\n",
    "            accs_max.append(np.max(part_res[index][dict_key][0])/100)\n",
    "            W.append(part_res[index][dict_key][1])\n",
    "    part_nets[lrs_inv[lr_idx]] = [[accs_mean, accs_max], W]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of the M partial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:12:32.239425Z",
     "iopub.status.busy": "2023-01-16T03:12:32.239032Z",
     "iopub.status.idle": "2023-01-16T03:12:32.256178Z",
     "shell.execute_reply": "2023-01-16T03:12:32.254321Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregation(run, lr, part_weights):\n",
    "    # Restore trained weights of partial models\n",
    "    weights1, weights2, weights3, bias1, bias2, bias3 = [],[],[],[], [], []\n",
    "    for i in range(num_batches):\n",
    "        weights1.append(part_weights[0])\n",
    "        bias1.append(part_weights[1])\n",
    "        weights2.append(part_weights[2])\n",
    "        bias2.append(part_weights[3])\n",
    "\n",
    "    # Net structure\n",
    "    aggr_net = Network()\n",
    "    aggr_net.add(FlattenLayer())\n",
    "    aggr_net.add(FCLayer(28*28, 200))\n",
    "    aggr_net.add(FCLayer(200, 10, outLayer=True))\n",
    "\n",
    "    # Compute the average of the weights\n",
    "    average_weights1 = np.mean(weights1, axis=0).astype(int)\n",
    "    average_bias1 = np.mean(bias1, axis=0).astype(int)\n",
    "    average_weights2 = np.mean(weights2, axis=0).astype(int)\n",
    "    average_bias2 = np.mean(bias2, axis=0).astype(int)\n",
    "\n",
    "    # Set the averaged weights\n",
    "    aggr_net.layers[1].weights = average_weights1\n",
    "    aggr_net.layers[1].bias = average_bias1\n",
    "    aggr_net.layers[2].weights = average_weights2\n",
    "    aggr_net.layers[2].bias = average_bias2\n",
    "\n",
    "    # Compute validation accuracy\n",
    "    val_acc = aggr_net.test(x_val, y_val)\n",
    "\n",
    "    return {str(lr)+'-'+str(run): [val_acc, [average_weights1, average_bias1, average_weights2, average_bias2]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:12:32.262973Z",
     "iopub.status.busy": "2023-01-16T03:12:32.262543Z",
     "iopub.status.idle": "2023-01-16T03:13:00.851915Z",
     "shell.execute_reply": "2023-01-16T03:13:00.849816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 28.579902172088623 s\n"
     ]
    }
   ],
   "source": [
    "# Parallelization of the aggregation procedure using joblib \n",
    "start_time = time.time()\n",
    "aggr_res = Parallel(n_jobs=-1)(delayed(aggregation)(run, lr, part_nets[lr][1][num_batches*run+batch]) for lr in lrs_inv for run in range(runs))\n",
    "end_time = time.time()\n",
    "print(\"Total time: \" + str(end_time-start_time) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:13:00.860093Z",
     "iopub.status.busy": "2023-01-16T03:13:00.859478Z",
     "iopub.status.idle": "2023-01-16T03:13:00.872376Z",
     "shell.execute_reply": "2023-01-16T03:13:00.870644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder results of aggregated networks\n",
    "aggr_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs, W = [], []\n",
    "    for run in range(runs):\n",
    "        index, dict_key = runs*lr_idx+run, str(lrs_inv[lr_idx])+'-'+str(run)\n",
    "        accs.append(aggr_res[index][dict_key][0]/100)\n",
    "        W.append(aggr_res[index][dict_key][1])\n",
    "    aggr_nets[lrs_inv[lr_idx]] = [accs, W]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:13:00.880113Z",
     "iopub.status.busy": "2023-01-16T03:13:00.879507Z",
     "iopub.status.idle": "2023-01-16T03:13:01.092448Z",
     "shell.execute_reply": "2023-01-16T03:13:01.090462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save aggregated nets\n",
    "with open(\"res/aggr\"+str(num_batches)+\".pkl\", \"wb\") as f:\n",
    "    pickle.dump(aggr_nets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T03:13:01.099440Z",
     "iopub.status.busy": "2023-01-16T03:13:01.099256Z",
     "iopub.status.idle": "2023-01-16T03:13:01.104803Z",
     "shell.execute_reply": "2023-01-16T03:13:01.103326Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save accuracies aggregated nets\n",
    "accs_aggr_nets = {}\n",
    "for lr in lrs_inv:\n",
    "    accs_aggr_nets[lr] = aggr_nets[lr][0]\n",
    "\n",
    "with open(\"res/accs/accs_aggr\"+str(num_batches)+\".pkl\", \"wb\") as f:\n",
    "    pickle.dump(accs_aggr_nets, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
